{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "\n",
    "##########################################\n",
    "\n",
    "\n",
    "\n",
    "def VGradient(data:np.ndarray,sigma,x:np.ndarray=None,coeffs:np.ndarray=None):\n",
    "    \"\"\"Compute the value of the qunatum potential generated by points data, and its gradient, at point x.\n",
    "    *inputs:*\n",
    "    data: a n-by-q numpy.ndarray with samples of the Parzen function (quantum wave function) Each row is a sample.\n",
    "    sigma: a numeric scalar representing the width the Gaussian around each sample\n",
    "    x: a m-by-1 numpy.ndarray of points where the values of the potential and the gradients are to be computed. each row is a point. if x is not given, data will be used instead.\n",
    "    coeffs: (optional) a n-by-1 numpy.ndarray of weights to each sample in data\n",
    "    *outputs:*\n",
    "    v: a m-by-1 numpy.ndarray of the values of the quantum potential function, for each point in x\n",
    "    dv: a m-by-q numpy.ndarray of the gradients of the quantum potential, for each point in x\"\"\"\n",
    "    \n",
    "    if x is None:\n",
    "        x = data.copy()\n",
    "    \n",
    "    if coeffs is None:\n",
    "        coeffs = np.ones((data.shape[0],))\n",
    "    \n",
    "        \n",
    "    twoSigmaSquared = 2*sigma**2\n",
    "        \n",
    "    data = data[np.newaxis,:,:]\n",
    "    x = x[:,np.newaxis,:]\n",
    "    differences = x-data\n",
    "    squaredDifferences = np.sum(np.square(differences),axis=2)\n",
    "    gaussian = np.exp(-(1/twoSigmaSquared)*squaredDifferences)\n",
    "    laplacian = np.sum(coeffs*gaussian*squaredDifferences,axis=1)\n",
    "    parzen = np.sum(coeffs*gaussian,axis=1)\n",
    "    v = 1 + (1/twoSigmaSquared)*laplacian/parzen\n",
    "\n",
    "    dv = -1*(1/parzen[:,np.newaxis])*np.sum(differences*((coeffs*gaussian)[:,:,np.newaxis])*(twoSigmaSquared*(v[:,np.newaxis,np.newaxis])-(squaredDifferences[:,:,np.newaxis])),axis=1)\n",
    "    \n",
    "    v = v-1\n",
    "    \n",
    "    return v, dv\n",
    "\n",
    "def SGradient(data:np.ndarray,sigma,x:np.ndarray=None,coeffs:np.ndarray=None):\n",
    "    \"\"\"Compute the value of the entropy generated by points data, and its gradient, at point x.\n",
    "    *inputs:*\n",
    "    data: a n-by-q numpy.ndarray with samples of the Parzen function. Each row is a sample.\n",
    "    sigma: a numeric scalar representing the width the Gaussian around each sample\n",
    "    x: a m-by-1 numpy.ndarray of points where the values of the entropy and the gradients are to be computed. each row is a point. if x is not given, data will be used instead.\n",
    "    coeffs: (optional) a n-by-1 numpy.ndarray of weights to each sample in data\n",
    "    *outputs:*\n",
    "    s: a m-by-1 numpy.ndarray of the values of the entropy, for each point in x\n",
    "    ds: a m-by-q numpy.ndarray of the gradients of the entropy, for each point in x\"\"\"\n",
    "    if x is None:\n",
    "        x = data.copy()\n",
    "        \n",
    "    if coeffs is None:\n",
    "        coeffs = np.ones((data.shape[0],))\n",
    "    \n",
    "    twoSigmaSquared = 2 * sigma ** 2\n",
    "    \n",
    "    data = data[np.newaxis, :, :]\n",
    "    x = x[:, np.newaxis, :]\n",
    "    differences = x - data\n",
    "    squaredDifferences = np.sum(np.square(differences), axis=2)\n",
    "    gaussian = np.exp(-(1 / twoSigmaSquared) * squaredDifferences)\n",
    "    laplacian = np.sum(coeffs*gaussian * squaredDifferences, axis=1)\n",
    "    parzen = np.sum(coeffs*gaussian, axis=1)\n",
    "    v = (1 / twoSigmaSquared) * laplacian / parzen\n",
    "    s = v + np.log(np.abs(parzen))\n",
    "    \n",
    "    ds = (1 / parzen[:, np.newaxis]) * np.sum(differences * ((coeffs*gaussian)[:, :, np.newaxis]) * (\n",
    "    twoSigmaSquared * (v[:, np.newaxis, np.newaxis]) - (squaredDifferences[:, :, np.newaxis])), axis=1)\n",
    "    \n",
    "    return s, ds\n",
    "\n",
    "def PGradient(data:np.ndarray,sigma,x:np.ndarray=None,coeffs:np.ndarray=None):\n",
    "    \"\"\"Compute the value of the Parzen function generated by points data, and its gradient, at point x.\n",
    "        *inputs:*\n",
    "        data: a n-by-q numpy.ndarray with samples of the Parzen function. Each row is a sample.\n",
    "        sigma: a numeric scalar representing the width the Gaussian around each sample\n",
    "        x: a m-by-1 numpy.ndarray of points where the values of the Parzen and the gradients are to be computed. each row is a point. if x is not given, data will be used instead.\n",
    "        coeffs: (optional) a n-by-1 numpy.ndarray of weights to each sample in data\n",
    "        *outputs:*\n",
    "        p: a m-by-1 numpy.ndarray of the values of the Parzen function, for each point in x\n",
    "        dp: a m-by-q numpy.ndarray of the gradients of the Parzen function, for each point in x\"\"\"\n",
    "    if x is None:\n",
    "        x = data.copy()\n",
    "        \n",
    "    if coeffs is None:\n",
    "        coeffs = np.ones((data.shape[0],))\n",
    "    \n",
    "    twoSigmaSquared = 2 * sigma ** 2\n",
    "    \n",
    "    data = data[np.newaxis, :, :]\n",
    "    x = x[:, np.newaxis, :]\n",
    "    differences = x - data\n",
    "    squaredDifferences = np.sum(np.square(differences), axis=2)\n",
    "    gaussian = np.exp(-(1 / twoSigmaSquared) * squaredDifferences)\n",
    "    p = np.sum(coeffs*gaussian,axis=1)\n",
    "    \n",
    "    dp = -1*np.sum(differences * ((coeffs*gaussian)[:, :, np.newaxis]) * twoSigmaSquared,axis=1)\n",
    "    \n",
    "    return p, dp\n",
    "\n",
    "def getApproximateParzen(data:np.ndarray,sigma,voxelSize):\n",
    "    \"\"\"compute samples of the approximate Parzen functon, and their weights\n",
    "    *inputs:*\n",
    "        data: a n-by-q numpy.ndarray with samples of the Parzen function. Each row is a sample.\n",
    "        sigma:  a numeric scalar representing the width the Gaussian around each sample.\n",
    "        voxelSize: size of the side of a (hyper-)voxel in q dimensions, such that each voxel will contain at most one data point.\n",
    "    *outputs:*\n",
    "        newData: a m-by-q numpy.ndarray with the new data points, at most one per voxel\n",
    "        coeffs: a m-by-q numpy.ndarray with the weights of each new data point\"\"\"\n",
    "    newData = uniqueRows(np.floor(data/voxelSize)*voxelSize+voxelSize/2)[0]\n",
    "    \n",
    "    nMat = np.exp(-1*distance.squareform(np.square(distance.pdist(newData)))/(4*sigma**2))\n",
    "    mMat = np.exp(-1 * np.square(distance.cdist(newData,data)) / (4 * sigma ** 2))\n",
    "    cMat = np.linalg.solve(nMat,mMat)\n",
    "    coeffs = np.sum(cMat,axis=1)\n",
    "    coeffs = data.shape[0]*coeffs/sum(coeffs)\n",
    "    \n",
    "    return newData,coeffs\n",
    "\n",
    "\n",
    "\n",
    "#######################################################3\n",
    "\n",
    "\n",
    "def uniqueRows(x):\n",
    "    \"\"\"return the unique rows of x, their indexes, the reverse indexes and the counts\"\"\"\n",
    "    y = np.ascontiguousarray(x).view(np.dtype((np.void, x.dtype.itemsize * x.shape[1])))\n",
    "    _, inds,indsInverse,counts = np.unique(y, return_index=True,return_inverse=True,return_counts=True)\n",
    "\n",
    "    xUnique = x[inds]\n",
    "    return xUnique,inds,indsInverse,counts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################\n",
    "\n",
    "\n",
    "def GradientDescent(data,sigma,repetitions=1,stepSize=None,clusteringType='v',recalculate=False,returnHistory=False,stopCondition=True,voxelSize=None):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    n = data.shape[0]\n",
    "\n",
    "    useApproximation = (voxelSize is not None)\n",
    "    \n",
    "    if stepSize is None:\n",
    "        stepSize = sigma/10\n",
    "    \n",
    "    if clusteringType == 'v':\n",
    "        gradientFunction = VGradient\n",
    "    elif clusteringType == 's':\n",
    "        gradientFunction = SGradient\n",
    "    else:\n",
    "        gradientFunction = PGradient\n",
    "\n",
    "    if useApproximation:\n",
    "        newData, coeffs = getApproximateParzen(data, sigma, voxelSize)\n",
    "    else:\n",
    "        coeffs = None\n",
    "\n",
    "    if recalculate:\n",
    "        if useApproximation:\n",
    "            x = np.vstack((data,newData))\n",
    "            data = x[data.shape[0]:]\n",
    "        else:\n",
    "            x = data\n",
    "    else:\n",
    "        if useApproximation:\n",
    "            x = data\n",
    "            data = newData\n",
    "        else:\n",
    "            x = data.copy()\n",
    "        \n",
    "        \n",
    "    if returnHistory:\n",
    "        xHistory = np.zeros((n,x.shape[1],repetitions+1))\n",
    "        xHistory[:,:,0] = x[:n,:].copy()\n",
    "        \n",
    "    if stopCondition:\n",
    "        prevX = x[:n].copy()\n",
    "\n",
    "    for i in range(repetitions):\n",
    "        if ((i>0) and (i%10==0)):\n",
    "            if stopCondition:\n",
    "                if np.all(np.linalg.norm(x[:n]-prevX,axis=1) < np.sqrt(3*stepSize**2)):\n",
    "                    i = i-1\n",
    "                    break\n",
    "                prevX = x[:n].copy()\n",
    "            \n",
    "        f,df = gradientFunction(data,sigma,x,coeffs)\n",
    "        df = df/np.linalg.norm(df,axis=1)[:,np.newaxis]\n",
    "        x[:] = x + stepSize*df\n",
    "\n",
    "        if returnHistory:\n",
    "            xHistory[:, :, i+1] = x[:n].copy()\n",
    "            \n",
    "    x = x[:n]\n",
    "\n",
    "    if returnHistory:\n",
    "        xHistory = xHistory[:,:,:(i+2)]\n",
    "        return x,xHistory\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def PerformFinalClustering(data,stepSize):\n",
    "    \"\"\"\"\"\"\n",
    "    clusters = np.zeros((data.shape[0]))\n",
    "    i = np.array([0])\n",
    "    c = 0\n",
    "    distances = distance.squareform(distance.pdist(data))\n",
    "    while i.shape[0]>0:\n",
    "        i = i[0]\n",
    "        inds = np.argwhere(clusters==0)\n",
    "        clusters[inds[distances[i,inds] <= 3*stepSize]] = c\n",
    "        c += 1\n",
    "        i = np.argwhere(clusters==0)\n",
    "    return clusters\n",
    "\n",
    "\n",
    "##############################################\n",
    "\n",
    "def displayClustering(xHistory,clusters=None):\n",
    "    \"\"\"\"\"\"\n",
    "    plt.ion()\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    if clusters is None:\n",
    "        clusters = np.zeros((xHistory.shape[0],))\n",
    "    if xHistory.shape[1] == 1:\n",
    "        plt.axes(aspect='equal')\n",
    "        sc = plt.scatter(xHistory[:,:,0],xHistory[:,:,0]*0,c=clusters,s=10)\n",
    "        plt.xlim((np.min(xHistory),np.max(xHistory)))\n",
    "        plt.ylim((-1,1))\n",
    "        for i in range(xHistory.shape[2]):\n",
    "            sc.set_offsets(xHistory[:, :, i])\n",
    "            plt.title('step #' + str(i) + '/' + str(xHistory.shape[2]-1))\n",
    "            plt.pause(0.05)\n",
    "    elif xHistory.shape[1] == 2:\n",
    "        plt.axes(aspect='equal')\n",
    "        sc = plt.scatter(xHistory[:, 0, 0], xHistory[:, 1, 0] , c=clusters, s=20)\n",
    "        plt.xlim((np.min(xHistory[:,0,:]), np.max(xHistory[:,0,:])))\n",
    "        plt.ylim((np.min(xHistory[:, 1, :]), np.max(xHistory[:, 1, :])))\n",
    "        for i in range(xHistory.shape[2]):\n",
    "            sc.set_offsets(xHistory[:, :, i])\n",
    "            plt.title('step #' + str(i) + '/' + str(xHistory.shape[2]-1))\n",
    "            plt.pause(0.2)\n",
    "    else:\n",
    "        if xHistory.shape[1] > 3:\n",
    "            pca = PCA(3)\n",
    "            pca.fit(xHistory[:,:,0])\n",
    "            newXHistory = np.zeros((xHistory.shape[0],3,xHistory.shape[2]))\n",
    "            for i in range(xHistory.shape[2]):\n",
    "                newXHistory[:,:,i] = pca.transform(xHistory[:,:,i])\n",
    "            xHistory = newXHistory\n",
    "\n",
    "        ax = plt.axes(aspect='equal',projection='3d')\n",
    "        sc = ax.scatter(xHistory[:, 0, 0], xHistory[:, 1, 0],xHistory[:, 2, 0], c=clusters, s=20)\n",
    "        ax.set_xlim((np.min(xHistory[:, 0, :]), np.max(xHistory[:, 0, :])))\n",
    "        ax.set_ylim((np.min(xHistory[:, 1, :]), np.max(xHistory[:, 1, :])))\n",
    "        ax.set_zlim((np.min(xHistory[:, 2, :]), np.max(xHistory[:, 2, :])))\n",
    "        for i in range(xHistory.shape[2]):\n",
    "            sc._offsets3d =  (np.ravel(xHistory[:, 0, i]),np.ravel(xHistory[:, 1, i]),np.ravel(xHistory[:, 2, i]))\n",
    "            plt.gcf().suptitle('step #' + str(i) + '/' + str(xHistory.shape[2]-1))\n",
    "            plt.pause(0.01)\n",
    "\n",
    "    plt.ioff()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data_iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris_target = data_iris.target\n",
    "\n",
    "### Replace this with dtm for CFPB data \n",
    "\n",
    "import os \n",
    "import string\n",
    "import pandas as pd \n",
    "\n",
    "#set working environment \n",
    "os.chdir(\"/home/spenser/Downloads/case_study\")\n",
    "\n",
    "#Import Data\n",
    "CFPB_1 = pd.read_csv(\"cfpb_1.csv\")\n",
    "\n",
    "CFPB_2 = pd.read_csv(\"cfpb_2.csv\", header = None)\n",
    "CFPB_2.columns = ['complaint_id', 'text']\n",
    "\n",
    "CFPB_3 = pd.read_csv(\"cfpb_3.csv\", header = None)\n",
    "CFPB_3.columns = ['complaint_id', 'text']\n",
    "\n",
    "CFPB_4 = pd.read_csv(\"cfpb_4.csv\", header = None)\n",
    "CFPB_4.columns = ['complaint_id', 'text']\n",
    "\n",
    "CFPB_5 = pd.read_csv(\"cfpb_5.csv\", header = None)\n",
    "CFPB_5.columns = ['complaint_id', 'text']\n",
    "\n",
    "CFPB_text = pd.concat([CFPB_1, CFPB_2, CFPB_3, CFPB_4, CFPB_5])\n",
    "\n",
    "file_no_text = pd.read_csv(\"cfpb_triage_case_study_notext.csv\")\n",
    "\n",
    "# Merging Case Study Files to Complaint IDs\n",
    "CFPB_Case_Study_Joined = file_no_text.merge(CFPB_text, on = 'complaint_id', how ='left')\n",
    "\n",
    "\n",
    "CFPB_Case_Study_Joined[\"text_lower\"] = CFPB_Case_Study_Joined[\"text\"].str.lower()\n",
    "\n",
    "CFPB_Case_Study_Joined[\"text_lower\"] = CFPB_Case_Study_Joined[\"text_lower\"].str.replace(r'\\nRevision: (\\d+)\\n', '') #remove digits\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "CFPB_Case_Study_Joined[\"text_clean\"] = CFPB_Case_Study_Joined['text_lower'].apply(remove_punctuations)  #remove punctuation\n",
    "\n",
    "#Adding this due to finding below that pre-cleansed text is corrupt. (contains cases like can t) . Remove single stand-alone characters. (\"a\", \"e\", etc)\n",
    "\n",
    "CFPB_Case_Study_Joined[\"text_clean\"] = CFPB_Case_Study_Joined[\"text_clean\"].str.replace(r'\\b(?<=)[a-z](?=)\\b', '') #remove single stand-alone characters.\n",
    "\n",
    "\n",
    "\n",
    "###############\n",
    "\n",
    "import gc\n",
    "\n",
    "# Remove stop words.\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "CFPB_Case_Study_Joined['text_clean_stopwords'] = CFPB_Case_Study_Joined['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)])) #stop word removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "Training epoch 1\n",
      "Training epoch 2\n",
      "Training epoch 3\n",
      "Training epoch 4\n",
      "Training epoch 5\n",
      "Training epoch 6\n",
      "Training epoch 7\n",
      "Training epoch 8\n",
      "Training epoch 9\n"
     ]
    }
   ],
   "source": [
    "#### Due to memory errors, using doc2vec approach. This will reduce dimensionality. I am not sure at this time if this\n",
    "### is appropriate for the quantum clustering methodology. \n",
    "\n",
    "import nltk\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "max_epochs = 10\n",
    "vec_size = 300\n",
    "alpha = 0.025\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "train_tagged = CFPB_Case_Study_Joined.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['text_clean_stopwords']), tags=[r.product_group]), axis=1)\n",
    "\n",
    "    \n",
    "NUM_VECTORS = 50\n",
    "\n",
    "vector_count = NUM_VECTORS\n",
    "model = Doc2Vec(size=vector_count, min_count=1, dm=0, iter=1,\n",
    "                dm_mean=1, dbow_words=1, workers=12)\n",
    "model.build_vocab(train_tagged)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print (\"Training epoch\", epoch)\n",
    "    model.train(train_tagged,  total_examples=model.corpus_count,  epochs=model.epochs)\n",
    "    model.alpha -= 0.02\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, 0.03, steps=20)) for doc in sents])\n",
    "    return targets, regressors\n",
    "\n",
    "\n",
    "y_train, X_train = vec_for_learning(model, train_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pd = pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#CFPB_Subset = CFPB_Case_Study_Joined[0:1000]\n",
    "\n",
    "\n",
    "##tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "#token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "#cv = CountVectorizer(lowercase=True,stop_words='english', ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "#text_counts= cv.fit_transform(CFPB_Subset[\"text_clean_stopwords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_counts_array = text_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x6919 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 59322 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data attributes for the Iris dataset.\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import preprocessing\n",
    "# load the Iris dataset\n",
    "\n",
    "\n",
    "# separate the data and target attributes\n",
    "X = np.array(X_train)\n",
    "# standardize the data attributes\n",
    "#standardized_X = preprocessing.scale(text_counts_array)\n",
    "#X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(268701, 300)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-a877acae8bfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvoxelSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxHistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrepetitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepetitions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstepSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstepSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclusteringType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclusteringType\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecalculate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecalculate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturnHistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturnHistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstopCondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopCondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvoxelSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvoxelSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerformFinalClustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstepSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-fbc7ae146020>\u001b[0m in \u001b[0;36mGradientDescent\u001b[0;34m(data, sigma, repetitions, stepSize, clusteringType, recalculate, returnHistory, stopCondition, voxelSize)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturnHistory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mxHistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrepetitions\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mxHistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import cluster\n",
    "\n",
    "\n",
    "sigma=0.55\n",
    "repetitions=100\n",
    "stepSize=0.1\n",
    "clusteringType='v'\n",
    "recalculate=False\n",
    "returnHistory=True\n",
    "stopCondition=True\n",
    "voxelSize = None\n",
    "\n",
    "x,xHistory = GradientDescent(X,sigma=sigma,repetitions=repetitions,stepSize=stepSize,clusteringType=clusteringType,recalculate=recalculate,returnHistory=returnHistory,stopCondition=stopCondition,voxelSize=voxelSize)\n",
    "\n",
    "clusters = PerformFinalClustering(x,stepSize)\n",
    "\n",
    "displayClustering(xHistory,clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-f21f9aad9290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcoeffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetApproximateParzen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoxelSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-fbc7ae146020>\u001b[0m in \u001b[0;36mgetApproximateParzen\u001b[0;34m(data, sigma, voxelSize)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mnewData\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mq\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mat\u001b[0m \u001b[0mmost\u001b[0m \u001b[0mone\u001b[0m \u001b[0mper\u001b[0m \u001b[0mvoxel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         coeffs: a m-by-q numpy.ndarray with the weights of each new data point\"\"\"\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mnewData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muniqueRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mvoxelSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvoxelSize\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvoxelSize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mnMat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquareform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "coeffs = getApproximateParzen(data_iris.data, sigma, voxelSize)\n",
    "f,df = VGradient(data_iris.data,sigma,x,coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
