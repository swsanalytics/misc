{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction/Embeddings of Classical Chinese poetry\n",
    "\n",
    "    - This notebook creates a process to produce embeddings from a cross-dynastic corpus of Chinese poems:\n",
    "     秦 (Qin), 汉(Han), 唐 (Tang), 南北朝 (Northern/Southern Dynasties), 宋 (Song), 元 (Yuan), 明 (Ming), 清 (Qing) \n",
    "     \n",
    "    - Embeddings create vectorized (numeric) representations for text, which exist in distributional\n",
    "      semantic embedding space. Since text data is high dimensional in nature (similar to genetic data, audio data, etc -- compression of the multi-dimensional corpus and distributional semantic models are needed in order to ascertain general (and latent) structures within the data.\n",
    "      \n",
    "      - [ This is an exercise in informational retrieval (IR) using machine learning and Chinese poems ]\n",
    "      \n",
    "      \n",
    "    - Embedded language has shown useful not only in information retrieval tasks (topic modeling, etc.) -- but\n",
    "      also as inputs to document classifiers and in natura language understanding for machines. As an extension\n",
    "      of the embeddings created here, a general adversarial network (GAN) is trained to generate Chinese poetry\n",
    "      non-deterministically and autonomously with seed words as inputs, capitalizing on latent structure within \n",
    "      the latent space geometry of the embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nǐhǎo\n"
     ]
    }
   ],
   "source": [
    "#pinyin and translation/transliteration support \n",
    "\n",
    "import pinyin \n",
    "print(pinyin.get('你好'))\n",
    "\n",
    "#Chinese character segmentation, tokenization, and dictionary features\n",
    "from chinese import ChineseAnalyzer\n",
    "\n",
    "#Efficient multi-core processing and progress bar utility \n",
    "import multiprocessing\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "#General python processing\n",
    "import pandas as pd \n",
    "import string\n",
    "import re\n",
    "\n",
    "##Baidu stopwords: Stopwords are very frequent in language and can interfere with information retrieval\n",
    "## this is a baidu library of stopwords so that they can be scrubbed from the poems -- the list is modern,\n",
    "## so there may be a mismatch*\n",
    "\n",
    "baidu_stopwords = pd.read_csv(\"/home/spenser/Poetry/stopwords/baidu_stopwords.txt\", header=None, encoding = 'utf-8', sep =\",\")\n",
    "baidu_stopwords.columns = ['baidu_sw']\n",
    " #https://github.com/goto456/stopwords.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Cross-Dynastic Poetry Corpus\n",
    "    -Each .csv file containing poems organized by dynastic era will be combined into a single\n",
    "     dataframe. This dataframe will be the input to NLP text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706638 Poems in the cross-Dynastic corpus\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>题目</th>\n",
       "      <th>朝代</th>\n",
       "      <th>作者</th>\n",
       "      <th>内容</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>三秦民谣</td>\n",
       "      <td>秦</td>\n",
       "      <td>无名氏</td>\n",
       "      <td>武功太白，去天三百。孤云两角，去天一握。山水险阻，黄金子午。蛇盘鸟栊，势与天通。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>巴谣歌</td>\n",
       "      <td>秦</td>\n",
       "      <td>阙名</td>\n",
       "      <td>神仙得者茅初成，驾龙上升入太清。时下玄洲戏赤城，继世而往在我盈，帝若学之腊嘉平。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>大招</td>\n",
       "      <td>汉</td>\n",
       "      <td>作者未详</td>\n",
       "      <td>青春受谢，白日昭只。春气奋发，万物遽只。冥淩浃行，魂无逃只。魂魄归徕！无远遥只。魂乎归徕！无...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>上邪</td>\n",
       "      <td>汉</td>\n",
       "      <td>两汉乐府</td>\n",
       "      <td>上邪。我欲与君相知。长命无绝衰。山无陵。江水为竭。冬雷震震夏雨雪。天地合。乃敢与君绝。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>孔雀东南飞 古诗为焦仲卿妻作</td>\n",
       "      <td>汉</td>\n",
       "      <td>两汉乐府</td>\n",
       "      <td>孔雀东南飞。五里一徘徊。十三能织素。十四学裁衣。十五弹箜篌。十六诵诗书。十七为君妇。心中常苦...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               题目 朝代    作者                                                 内容\n",
       "0            三秦民谣  秦   无名氏           武功太白，去天三百。孤云两角，去天一握。山水险阻，黄金子午。蛇盘鸟栊，势与天通。\n",
       "1             巴谣歌  秦    阙名           神仙得者茅初成，驾龙上升入太清。时下玄洲戏赤城，继世而往在我盈，帝若学之腊嘉平。\n",
       "2              大招  汉  作者未详  青春受谢，白日昭只。春气奋发，万物遽只。冥淩浃行，魂无逃只。魂魄归徕！无远遥只。魂乎归徕！无...\n",
       "3              上邪  汉  两汉乐府        上邪。我欲与君相知。长命无绝衰。山无陵。江水为竭。冬雷震震夏雨雪。天地合。乃敢与君绝。\n",
       "4  孔雀东南飞 古诗为焦仲卿妻作  汉  两汉乐府  孔雀东南飞。五里一徘徊。十三能织素。十四学裁衣。十五弹箜篌。十六诵诗书。十七为君妇。心中常苦..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "os.chdir(\"/home/spenser/Poetry/\")\n",
    "\n",
    "\n",
    "qin_all = pd.read_csv(\"秦.csv\")\n",
    "\n",
    "han_all = pd.read_csv(\"汉.csv\")\n",
    "\n",
    "tang_all = pd.read_csv(\"唐.csv\")\n",
    "\n",
    "northern_southern_dyn_all = pd.read_csv(\"南北朝.csv\")\n",
    "\n",
    "song1 = pd.read_csv(\"宋_1.csv\")\n",
    "song2 = pd.read_csv(\"宋_2.csv\")\n",
    "song3 = pd.read_csv(\"宋_3.csv\")\n",
    "song4 = pd.read_csv(\"宋_4.csv\")\n",
    "song_all = pd.concat([song1, song2, song3, song4])\n",
    "\n",
    "yuan_all = pd.read_csv(\"元.csv\")\n",
    "\n",
    "ming1 = pd.read_csv(\"明_1.csv\")\n",
    "ming2 = pd.read_csv(\"明_1.csv\")\n",
    "ming3 = pd.read_csv(\"明_1.csv\")\n",
    "ming4 = pd.read_csv(\"明_1.csv\")\n",
    "ming_all = pd.concat([ming1, ming2, ming3, ming4])\n",
    "\n",
    "qing1 = pd.read_csv(\"清_1.csv\")\n",
    "qing2 = pd.read_csv(\"清_2.csv\")\n",
    "qing_all = pd.concat([qing1, qing2])\n",
    "\n",
    "###unify\n",
    "\n",
    "all_poems = pd.concat([qin_all, han_all, tang_all, northern_southern_dyn_all,\n",
    "                      song_all, yuan_all, ming_all, qing_all]).reset_index(drop=True)\n",
    "\n",
    "print(len(all_poems), \"Poems in the cross-Dynastic corpus\")\n",
    "\n",
    "\n",
    "all_poems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "宋      287114\n",
       "明      237912\n",
       "清       90088\n",
       "唐       49195\n",
       "元       37375\n",
       "南北朝      4586\n",
       "汉         363\n",
       "秦           2\n",
       "许梦青         1\n",
       "Name: 朝代, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counts of Poems by Dynastic Era \n",
    "#So few from earlier history in the corpus!\n",
    "#This will mostly be a comparison of song - Qing.\n",
    "\n",
    "all_poems[\"朝代\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poem Text Preprocessing\n",
    "\n",
    "    - This utilizes the pre-built segmenter and tokenizer for Chinese language available\n",
    "      from pip install chinese. \n",
    "      \n",
    "    -Segmentation is necessary for Chinese language inputs without spaces -- segmentation is done\n",
    "     algorithmically within the ChineseAnalyzer module so is subject to error. \n",
    "     \n",
    "    -Spaces are necessary in NLP for token (word unit) vectorization (creating numerical representations of unique words for algorithmic processing and pattern organization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "武功太白，去天三百 Example of First Line\n",
      "Full Poem, Example First Line \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['武功太白，去天三百', '孤云两角，去天一握', '山水险阻，黄金子午', '蛇盘鸟栊，势与天通', '']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(all_poems[\"内容\"].iloc[0].split('。')[0] , \"Example of First Line\")\n",
    "\n",
    "print('Full Poem, Example First Line ')\n",
    "all_poems[\"内容\"].iloc[0].split('。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original': '武功太白，去天三百',\n",
      " 'parsed': [{'dict_data': [{'definitions': ['Wugong County in Xianyang '\n",
      "                                            '咸陽|咸阳[Xian2 yang2], Shaanxi'],\n",
      "                            'kind': 'Simplified',\n",
      "                            'match': '武功',\n",
      "                            'pinyin': ['Wu3', 'gong1']},\n",
      "                           {'definitions': ['martial art',\n",
      "                                            'military accomplishments',\n",
      "                                            '(Peking opera) martial arts '\n",
      "                                            'feats'],\n",
      "                            'kind': 'Simplified',\n",
      "                            'match': '武功',\n",
      "                            'pinyin': ['wu3', 'gong1']}],\n",
      "             'token': ('武功', 0, 2)},\n",
      "            {'dict_data': [{'definitions': ['Taibai County in Baoji 寶雞|宝鸡[Bao3 '\n",
      "                                            'ji1], Shaanxi',\n",
      "                                            'Venus'],\n",
      "                            'kind': 'Simplified',\n",
      "                            'match': '太白',\n",
      "                            'pinyin': ['Tai4', 'bai2']}],\n",
      "             'token': ('太白', 2, 4)},\n",
      "            {'dict_data': [{'definitions': None,\n",
      "                            'kind': 'Simplified',\n",
      "                            'match': '，',\n",
      "                            'pinyin': None}],\n",
      "             'token': ('，', 4, 5)},\n",
      "            {'dict_data': [{'definitions': ['to go',\n",
      "                                            'to go to (a place)',\n",
      "                                            '(of a time etc) last',\n",
      "                                            'just passed',\n",
      "                                            'to send',\n",
      "                                            'to remove',\n",
      "                                            'to get rid of',\n",
      "                                            'to reduce',\n",
      "                                            'to be apart from in space or time',\n",
      "                                            'to die (euphemism)',\n",
      "                                            'to play (a part)',\n",
      "                                            '(when used either before or after '\n",
      "                                            'a verb) to go in order to do sth',\n",
      "                                            '(after a verb of motion indicates '\n",
      "                                            'movement away from the speaker)',\n",
      "                                            '(used after certain verbs to '\n",
      "                                            'indicate detachment or '\n",
      "                                            'separation)'],\n",
      "                            'kind': 'Simplified',\n",
      "                            'match': '去',\n",
      "                            'pinyin': ['qu4']}],\n",
      "             'token': ('去', 5, 6)},\n",
      "            {'dict_data': [{'definitions': ['day', 'sky', 'heaven'],\n",
      "                            'kind': 'Simplified',\n",
      "                            'match': '天',\n",
      "                            'pinyin': ['tian1']}],\n",
      "             'token': ('天', 6, 7)},\n",
      "            {'dict_data': [{'definitions': None,\n",
      "                            'kind': 'Simplified',\n",
      "                            'match': '三百',\n",
      "                            'pinyin': None}],\n",
      "             'token': ('三百', 7, 9)}]}\n"
     ]
    }
   ],
   "source": [
    "from chinese import ChineseAnalyzer\n",
    "analyzer = ChineseAnalyzer()\n",
    "result = analyzer.parse(all_poems[\"内容\"].iloc[0].split('。')[0]\n",
    ")\n",
    "result.tokens()\n",
    "result.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Multiprocessing process to distribute ChineseAnalyzer across CPUs at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analyzer = ChineseAnalyzer()\n",
    "\n",
    "#Distribute ChineseAnalyzer function to segment and tokenize each poem across multiple CPUs. \n",
    "\n",
    "def segment_chinese(input_text_df):\n",
    "   \n",
    "    #Instantiate progress bar so this doesn't get stuck in an infinite loop!\n",
    "    \n",
    "    result = analyzer.parse(input_text_df)\n",
    "    tokens = result.tokens()\n",
    "    #scruntch segmented tokens back into a string for input to vectorizers\n",
    "    re_string = ' '.join(tokens).replace(' , ',',').replace(' .','.').replace(' !','!').replace(' 。', '。').replace(' ?', '?')\n",
    "    #dictionary_items = result.pformat()\n",
    "    return re_string\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "p = Pool(8)\n",
    "\n",
    "\n",
    "all_poems[\"segmented\"] = p.map(segment_chinese, all_poems[\"内容\"].astype('unicode'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print examples of segmentation output\n",
      "武功 太白 ， 去 天 三百。 孤云 两角 ， 去 天一 握。 山水 险阻 ， 黄金 子午。 蛇盘 鸟 栊 ， 势 与 天通。\n",
      " \n",
      "------------------\n",
      "神仙 得者 茅 初成 ， 驾龙 上升 入太清。 时下 玄洲 戏 赤城 ， 继世而往 在 我盈 ， 帝若学 之腊嘉平。\n",
      " \n",
      "------------------\n",
      "青春 受谢 ， 白日 昭 只。 春气 奋发 ， 万物 遽 只。 冥 淩 浃 行 ， 魂 无 逃 只。 魂魄 归徕 ！ 无远遥 只。 魂乎 归徕 ！ 无东 无西无南 无北 只。 东 有 大海 ， 溺水 浟 浟 只。 螭 龙 并 流 ， 上下 悠悠 只。 雾雨淫 淫 ， 白皓胶 只。 魂乎 无东 ！ 汤谷? 只。 魂乎 无南 ！ 南有 炎火 千里 ， 蝮蛇 蜒 只。 山林 险隘 ， 虎豹 蜿只。 鰅 鳙 短 狐 ， 王 虺 骞 只。 魂乎 无南 ！ 蜮 伤 躬 只。 魂乎 无西 ！ 西方 流沙 ， 漭 洋洋 只。 豕 首 纵目 ， 被 发 鬤 只。 长爪 踞 牙 ， 诶 笑 狂 只。 魂乎 无西 ！ 多害 伤 只。 魂乎 无北 ！ 北有 寒山 ， 逴 龙 赩 只。 代水 不可 涉 ， 深不可测 只。 天白颢颢 ， 寒凝凝只。 魂乎 无往 ！ 盈 北极 只。 魂魄 归徕 ！ 閒以静 只。 自 恣 荆楚 ， 安以定 只。 逞志究 欲 ， 心意 安只。 穷身 永乐 ， 年寿延 只。 魂乎 归徕 ！ 乐不可言 只。 五谷 六仞 ， 设 菰 粱 只。 鼎 臑 盈望 ， 和 致芳 只。 内 鸧 鸽 鹄 ， 味 豺 羹 只。 魂乎 归徕 ！ 恣所尝 只。 鲜 蠵 甘鸡 ， 和 楚酪 只。 醢 豚 苦 狗 ， 脍 苴 莼 只。 吴酸蒿 蒌 ， 不 沾 薄 只。 魂 兮 归徕 ！ 恣所择 只。 炙 鸹 烝 凫 ， 煔 鹑 陈 只。 煎 鰿? 雀 ， 遽 爽存 只。 魂乎 归徕 ！ 丽以 先 只。 四 酎 并 孰 ， 不 歰 嗌 只。 清馨 冻 饮 ， 不 歠 役 只。 吴 醴白 糵 ， 和 楚 沥 只。 魂乎 归徕 ！ 不遽 惕 只。 代秦 郑卫 ， 鸣竽张 只。 伏戏 《 驾辩 》 ， 楚 《 劳商 》 只。 讴 和 《 扬 阿 》 ， 赵箫倡 只。 魂乎 归徕 ！ 定 空桑 只。 二八 接舞 ， 投 诗赋 只。 叩钟 调磬 ， 娱人乱 只。 四上 竞气 ， 极声 变 只。 魂乎 归徕 ！ 听歌 撰 只。 朱唇皓齿 ， 嫭 以 姱 只。 比德 好閒 ， 习以 都 只。 丰肉 微骨 ， 调以 娱只。 魂乎 归徕 ！ 安以 舒只。 嫭 目宜 笑 ， 娥眉 曼只。 容则 秀雅 ， 稚 朱颜 只。 魂乎 归徕 ！ 静以 安 只。 姱 修滂浩 ， 丽以 佳 只。 曾颊 倚 耳 ， 曲眉规 只。 滂 心绰态 ， 姣丽 施只。 小 腰秀颈 ， 若 鲜卑 只。 魂乎 归徕 ！ 思怨 移 只。 易 中利心 ， 以 动作 只。 粉白黛黑 ， 施 芳泽 只。 长袂 拂面 ， 善 留客 只。 魂乎 归徕 ！ 以 娱昔 只。 青色 直眉 ， 美目 媔 只。 靥 辅奇牙 ， 宜 笑 嘕 只。 丰肉 微骨 ， 体便娟 只。 魂乎 归徕 ！ 恣所 便 只。 夏屋 广大 ， 沙堂秀 只。 南房 小坛 ， 观绝 霤 只。 曲屋 步 壛 ， 宜扰 畜 只。 腾驾 步游 ， 猎春 囿 只。 琼毂错衡 ， 英华 假 只。 茝 兰 桂树 ， 郁弥路 只。 魂乎 归徕 ！ 恣志虑 只。 孔雀 盈园 ， 畜鸾 皇只。 鹍 鸿 群晨 ， 杂 鹙 鸧 只。 鸿鹄 代游 ， 曼 鹔 鹴 只。 魂乎 归徕 ！ 凤皇翔 只。 曼泽怡面 ， 血 气盛 只。 永宜厥 身 ， 保 寿命 只。 室家 盈廷 ， 爵禄 盛只。 魂乎 归徕 ！ 居室 定 只。 接径 千里 ， 出若云 只。 三圭 重侯 ， 听类 神 只。 察笃夭隐 ， 孤寡 存只。 魂 兮 归徕 ！ 正始 昆只。 田邑千 畛 ， 人阜昌 只。 美冒 众流 ， 德泽章 只。 先威 后文 ， 善美明 只。 魂乎 归徕 ！ 赏罚 当 只。 名声 若日 ， 照 四海 只。 德誉 配天 ， 万民 理只。 北至 幽陵 ， 南交 阯 只。 西薄 羊肠 ， 东穷海 只。 魂乎 归徕 ！ 尚 贤士 只。 发政献行 ， 禁 苛暴 只。 举杰压 陛 ， 诛 讥 罢 只。 直赢 在位 ， 近禹 麾 只。 豪杰 执政 ， 流泽施 只。 魂乎 归徕 ！ 国家 为 只。 雄雄 赫赫 ， 天德 明只。 三公 穆穆 ， 登降堂 只。 诸侯 毕极 ， 立 九卿 只。 昭质 既设 ， 大侯张 只。 执弓 挟 矢 ， 揖 辞让 只。 魂乎 归徕 ！ 尚 三王 只。\n"
     ]
    }
   ],
   "source": [
    "print('print examples of segmentation output')\n",
    "print(all_poems[\"segmented\"][0])\n",
    "print(' ')\n",
    "print('------------------')\n",
    "print(all_poems[\"segmented\"][1])\n",
    "print(' ')\n",
    "print('------------------')\n",
    "print(all_poems[\"segmented\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punctuation(text_input):\n",
    "    text_input = text_input\n",
    "    for punctuation in \"！。，《》\":\n",
    "    \n",
    "        text_input = re.sub(punctuation, \"\", text_input)\n",
    "        \n",
    "    return text_input\n",
    "\n",
    "p = Pool(8)\n",
    "all_poems[\"segmented_punctuation_removed\"] = p.map(remove_punctuation, all_poems[\"segmented\"] )\n",
    "\n",
    "p.terminate()\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spenser/anaconda3/envs/loss_landscape/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Without setting parameters to eliminate the most common, and less frequent (max_df, min_df)\n",
    "#terms - the count vectorizer produces too large of a vocabulary\n",
    "\n",
    "#It could be that the segmenter is finding many unique terms that are highly intrinstic to\n",
    "#chinese language -- e.g., chengyu or idiomatic type 4 character sequences.\n",
    "    ### Note: This is an assumption that will have to be explored , but is not in the scope of\n",
    "    ###       the present notebook. \n",
    "\n",
    "vectorizer = TfidfVectorizer( max_df=.95, min_df=5, norm='l2', use_idf=True , stop_words=list(baidu_stopwords['baidu_sw']))\n",
    "poems_vectorized = vectorizer.fit_transform(all_poems[\"segmented_punctuation_removed\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features = pd.DataFrame.from_dict(vectorizer.vocabulary_, orient='index', columns = ['counts'])\n",
    "tfidf_features[\"words\"] = tfidf_features.index\n",
    "tfidf_features = tfidf_features.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706638 poems in the vectorized corpus\n",
      "282475 features/words in the vectorized corpus\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105264</th>\n",
       "      <td>282474</td>\n",
       "      <td>龟龙出</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34404</th>\n",
       "      <td>282473</td>\n",
       "      <td>龟龙</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211575</th>\n",
       "      <td>282472</td>\n",
       "      <td>龟龄鹤算</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126110</th>\n",
       "      <td>282471</td>\n",
       "      <td>龟龄</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278452</th>\n",
       "      <td>282470</td>\n",
       "      <td>龟鼎</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167727</th>\n",
       "      <td>4</td>\n",
       "      <td>一丁不识</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58404</th>\n",
       "      <td>3</td>\n",
       "      <td>一丁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158541</th>\n",
       "      <td>2</td>\n",
       "      <td>一一记</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266606</th>\n",
       "      <td>1</td>\n",
       "      <td>一一分</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5626</th>\n",
       "      <td>0</td>\n",
       "      <td>一一</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>282475 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        counts words\n",
       "105264  282474   龟龙出\n",
       "34404   282473    龟龙\n",
       "211575  282472  龟龄鹤算\n",
       "126110  282471    龟龄\n",
       "278452  282470    龟鼎\n",
       "...        ...   ...\n",
       "167727       4  一丁不识\n",
       "58404        3    一丁\n",
       "158541       2   一一记\n",
       "266606       1   一一分\n",
       "5626         0    一一\n",
       "\n",
       "[282475 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(poems_vectorized.shape[0], \"poems in the vectorized corpus\")\n",
    "print(poems_vectorized.shape[1], \"features/words in the vectorized corpus\")\n",
    "\n",
    "tfidf_features.sort_values(by='counts', ascending=False)\n",
    "\n",
    "###龟龄鹤算 is so common! Something about an old wise tortise. Why is this so pervasive?\n",
    "### Check for all influential chengyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-dfb93ec0f5cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msvd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msvd_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoems_vectorized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVD (Compressed) Components Cumulatively Explain\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"% Variance\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/loss_landscape/lib/python3.6/site-packages/sklearn/decomposition/_truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    178\u001b[0m             U, Sigma, VT = randomized_svd(X, self.n_components,\n\u001b[1;32m    179\u001b[0m                                           \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                                           random_state=random_state)\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown algorithm %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/loss_landscape/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_svd\u001b[0;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     Q = randomized_range_finder(M, n_random, n_iter,\n\u001b[0;32m--> 348\u001b[0;31m                                 power_iteration_normalizer, random_state)\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;31m# project M to the (k + p) dimensional space using the basis vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/loss_landscape/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_range_finder\u001b[0;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LU'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'QR'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/loss_landscape/lib/python3.6/site-packages/scipy/linalg/decomp_lu.py\u001b[0m in \u001b[0;36mlu\u001b[0;34m(a, permute_l, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0moverwrite_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverwrite_a\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_datacopied\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mflu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_flinalg_funcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpermute_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         raise ValueError('illegal value in %d-th argument of '\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=500, random_state=42)\n",
    "\n",
    "svd_components = svd.fit_transform(poems_vectorized)\n",
    "\n",
    "print(\"SVD (Compressed) Components Cumulatively Explain\", \" \", sum(svd.explained_variance_ratio_)*100, \"% Variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# 3D Plot\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax3D = fig.add_subplot(111, projection='3d')\n",
    "ax3D.scatter(svd_components[:,0], svd_components[:,1], svd_components[:,2], s=3, c=pd.Categorical(all_poems[\"朝代\"]).codes, marker='o')  \n",
    "\n",
    "#plt.scatter(svd_components[:,0], svd_components[:,1], c=pd.Categorical(all_poems[\"朝代\"]).codes)\n",
    "\n",
    "#plt.xlim(0.0, 1.0)\n",
    "#plt.ylim(0.0, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Dynastic Corpus - Uniform Manifold Approximation (UMAP) to Vector Space\n",
    "    - Several vectorization methods will be explored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "\n",
    "embedding = umap.UMAP(n_components=2, metric='cosine').fit(poems_vectorized[0:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For interactive plotting use\n",
    "# f = umap.plot.interactive(embedding, labels=dataset.target, hover_data=hover_df, point_size=1)\n",
    "# show(f)\n",
    "f = umap.plot.points(embedding, labels=poems_all['朝代'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loss_landscape",
   "language": "python",
   "name": "loss_landscape"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
