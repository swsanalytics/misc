{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KATE\n",
    "\n",
    "from keras_utils import Dense_tied, KCompetitive, contractive_loss, CustomModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replaces io.utils\n",
    "#Check AA lab compatibiltiy\n",
    "'''\n",
    "Created on Nov, 2016\n",
    "\n",
    "@author: hugo\n",
    "\n",
    "'''\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import json\n",
    "#import cPickle as pickle\n",
    "import pickle as pickle\n",
    "import marshal as m\n",
    "\n",
    "\n",
    "def dump_marshal(data, path_to_file):\n",
    "    try:\n",
    "        with open(path_to_file, 'w') as f:\n",
    "            m.dump(data, f)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "def load_marshal(path_to_file):\n",
    "    try:\n",
    "        with open(path_to_file, 'r') as f:\n",
    "            data = m.load(f)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    return data\n",
    "\n",
    "def dump_pickle(data, path_to_file):\n",
    "    try:\n",
    "        with open(path_to_file, 'w') as f:\n",
    "            pickle.dump(data, f)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "def load_pickle(path_to_file):\n",
    "    try:\n",
    "        with open(path_to_file, 'r') as f:\n",
    "            data = pickle.load(f)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    return data\n",
    "\n",
    "def dump_json(data, file):\n",
    "    try:\n",
    "        with open(file, 'w') as datafile:\n",
    "            json.dump(data, datafile)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "def load_json(file):\n",
    "    try:\n",
    "        with open(file, 'r') as datafile:\n",
    "            data = json.load(datafile)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    return data\n",
    "\n",
    "def write_file(data, file):\n",
    "    try:\n",
    "        with open(file, 'w') as datafile:\n",
    "            for line in data:\n",
    "                datafile.write(' '.join(line) + '\\n')\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "def load_file(file, float_=False):\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file, 'r') as datafile:\n",
    "            for line in datafile:\n",
    "                content = line.strip('\\n').split()\n",
    "                if float_:\n",
    "                    content = [float(x) for x in content]\n",
    "                data.append(content)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Replaces __main__.op.utils\n",
    "\n",
    "'''\n",
    "Created on Nov, 2016\n",
    "\n",
    "@author: hugo\n",
    "\n",
    "'''\n",
    "from __future__ import absolute_import\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_ranks(x):\n",
    "    \"\"\"Given a list of items, return a list(in ndarray type) of ranks.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    index = list(zip(*sorted(list(enumerate(x)), key=lambda d:d[1], reverse=True))[0])\n",
    "    rank = np.zeros(n)\n",
    "    rank[index] = range(1, n + 1)\n",
    "    return rank\n",
    "\n",
    "def revdict(d):\n",
    "    \"\"\"\n",
    "    Reverse a dictionary mapping.\n",
    "    When two keys map to the same value, only one of them will be kept in the\n",
    "    result (which one is kept is arbitrary).\n",
    "    \"\"\"\n",
    "    return dict((v, k) for (k, v) in d.iteritems())\n",
    "\n",
    "def l1norm(x):\n",
    "    return x / sum([np.abs(y) for y in x])\n",
    "\n",
    "def vecnorm(vec, norm, epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Scale a vector to unit length. The only exception is the zero vector, which\n",
    "    is returned back unchanged.\n",
    "    \"\"\"\n",
    "    if norm not in ('prob', 'max1', 'logmax1'):\n",
    "        raise ValueError(\"'%s' is not a supported norm. Currently supported norms include 'prob',\\\n",
    "             'max1' and 'logmax1'.\" % norm)\n",
    "\n",
    "    if isinstance(vec, np.ndarray):\n",
    "        vec = np.asarray(vec, dtype=float)\n",
    "        if norm == 'prob':\n",
    "            veclen = np.sum(np.abs(vec)) + epsilon * len(vec) # smoothing\n",
    "        elif norm == 'max1':\n",
    "            veclen = np.max(vec) + epsilon\n",
    "        elif norm == 'logmax1':\n",
    "            vec = np.log10(1. + vec)\n",
    "            veclen = np.max(vec) + epsilon\n",
    "        if veclen > 0.0:\n",
    "            return (vec + epsilon) / veclen\n",
    "        else:\n",
    "            return vec\n",
    "    else:\n",
    "        raise ValueError('vec should be ndarray, found: %s' % type(vec))\n",
    "\n",
    "def unitmatrix(matrix, norm='l2', axis=1):\n",
    "    if norm == 'l1':\n",
    "        maxtrixlen = np.sum(np.abs(matrix), axis=axis)\n",
    "    if norm == 'l2':\n",
    "        maxtrixlen = np.linalg.norm(matrix, axis=axis)\n",
    "\n",
    "    if np.any(maxtrixlen <= 0):\n",
    "        return matrix\n",
    "    else:\n",
    "        maxtrixlen = maxtrixlen.reshape(1, len(maxtrixlen)) if axis == 0 else maxtrixlen.reshape(len(maxtrixlen), 1)\n",
    "        return matrix / maxtrixlen\n",
    "\n",
    "def add_gaussian_noise(X, corruption_ratio, range_=[0, 1]):\n",
    "    X_noisy = X + corruption_ratio * np.random.normal(loc=0.0, scale=1.0, size=X.shape)\n",
    "    X_noisy = np.clip(X_noisy, range_[0], range_[1])\n",
    "\n",
    "    return X_noisy\n",
    "\n",
    "def add_masking_noise(X, fraction):\n",
    "    assert fraction >= 0 and fraction <= 1\n",
    "    X_noisy = np.copy(X)\n",
    "    nrow, ncol = X.shape\n",
    "    n = int(ncol * fraction)\n",
    "    for i in range(nrow):\n",
    "        idx_noisy  = np.random.choice(ncol, n, replace=False)\n",
    "        X_noisy[i, idx_noisy] = 0\n",
    "\n",
    "    return X_noisy\n",
    "\n",
    "def add_salt_pepper_noise(X, fraction):\n",
    "    assert fraction >= 0 and fraction <= 1\n",
    "    X_noisy = np.copy(X)\n",
    "    nrow, ncol = X.shape\n",
    "    n = int(ncol * fraction)\n",
    "    for i in range(nrow):\n",
    "        idx_noisy  = np.random.choice(ncol, n, replace=False)\n",
    "        X_noisy[i, idx_noisy] = np.random.binomial(1, .5, n)\n",
    "\n",
    "    return X_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Replaced testing.visualize\n",
    "\n",
    "'''\n",
    "Created on Dec, 2016\n",
    "\n",
    "@author: hugo\n",
    "\n",
    "'''\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib as mpl\n",
    "mpl.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, MultipleLocator\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import interpolate\n",
    "\n",
    "\n",
    "class neural_net_visualizer(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def heatmap(data, save_file='heatmap.png'):\n",
    "    ax = plt.figure().gca()\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(5))\n",
    "    plt.pcolor(data, cmap=plt.cm.jet)\n",
    "    plt.savefig(save_file)\n",
    "    # plt.show()\n",
    "\n",
    "def word_cloud(word_embedding_matrix, vocab, s, save_file='scatter.png'):\n",
    "    words = [(i, vocab[i]) for i in s]\n",
    "    model = TSNE(n_components=2, random_state=0)\n",
    "    #Note that the following line might use a good chunk of RAM\n",
    "    tsne_embedding = model.fit_transform(word_embedding_matrix)\n",
    "    words_vectors = tsne_embedding[np.array([item[1] for item in words])]\n",
    "\n",
    "    plt.subplots_adjust(bottom = 0.1)\n",
    "    plt.scatter(\n",
    "        words_vectors[:, 0], words_vectors[:, 1], marker='o', cmap=plt.get_cmap('Spectral'))\n",
    "\n",
    "    for label, x, y in zip(s, words_vectors[:, 0], words_vectors[:, 1]):\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            xy=(x, y), xytext=(-20, 20),\n",
    "            textcoords='offset points', ha='right', va='bottom',\n",
    "            fontsize=20,\n",
    "            # bbox=dict(boxstyle='round,pad=1.', fc='yellow', alpha=0.5),\n",
    "            arrowprops=dict(arrowstyle = '<-', connectionstyle='arc3,rad=0')\n",
    "            )\n",
    "    plt.show()\n",
    "    # plt.savefig(save_file)\n",
    "\n",
    "def plot_tsne(doc_codes, doc_labels, classes_to_visual, save_file):\n",
    "    # markers = [\"D\", \"p\", \"*\", \"s\", \"d\", \"8\", \"^\", \"H\", \"v\", \">\", \"<\", \"h\", \"|\"]\n",
    "    markers = [\"o\", \"v\", \"8\", \"s\", \"p\", \"*\", \"h\", \"H\", \"+\", \"x\", \"D\"]\n",
    "    plt.rc('legend',**{'fontsize':30})\n",
    "    classes_to_visual = list(set(classes_to_visual))\n",
    "    C = len(classes_to_visual)\n",
    "    while True:\n",
    "        if C <= len(markers):\n",
    "            break\n",
    "        markers += markers\n",
    "\n",
    "    class_ids = dict(zip(classes_to_visual, range(C)))\n",
    "\n",
    "    if isinstance(doc_codes, dict) and isinstance(doc_labels, dict):\n",
    "        codes, labels = zip(*[(code, doc_labels[doc]) for doc, code in doc_codes.items() if doc_labels[doc] in classes_to_visual])\n",
    "    else:\n",
    "        codes, labels = doc_codes, doc_labels\n",
    "\n",
    "    X = np.r_[list(codes)]\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    X = tsne.fit_transform(X)\n",
    "\n",
    "    plt.figure(figsize=(10, 10), facecolor='white')\n",
    "\n",
    "    for c in classes_to_visual:\n",
    "        idx = np.array(labels) == c\n",
    "        # idx = get_indices(labels, c)\n",
    "        plt.plot(X[idx, 0], X[idx, 1], linestyle='None', alpha=1, marker=markers[class_ids[c]],\n",
    "                        markersize=10, label=c)\n",
    "    legend = plt.legend(loc='upper right', shadow=True)\n",
    "    # plt.title(\"tsne\")\n",
    "    # plt.savefig(save_file)\n",
    "    plt.savefig(save_file, format='eps', dpi=2000)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_tsne_3d(doc_codes, doc_labels, classes_to_visual, save_file, maker_size=None, opaque=None):\n",
    "    markers = [\"D\", \"p\", \"*\", \"s\", \"d\", \"8\", \"^\", \"H\", \"v\", \">\", \"<\", \"h\", \"|\"]\n",
    "    plt.rc('legend',**{'fontsize':20})\n",
    "    colors = ['r', 'b', 'g', 'c', 'm', 'y', 'k']\n",
    "    C = len(classes_to_visual)\n",
    "    while True:\n",
    "        if C <= len(markers):\n",
    "            break\n",
    "        markers += markers\n",
    "    while True:\n",
    "        if C <= len(colors):\n",
    "            break\n",
    "        colors += colors\n",
    "\n",
    "    class_ids = dict(zip(classes_to_visual, range(C)))\n",
    "\n",
    "    if isinstance(doc_codes, dict) and isinstance(doc_labels, dict):\n",
    "        codes, labels = zip(*[(code, doc_labels[doc]) for doc, code in doc_codes.items() if doc_labels[doc] in classes_to_visual])\n",
    "    else:\n",
    "        codes, labels = doc_codes, doc_labels\n",
    "\n",
    "    X = np.r_[list(codes)]\n",
    "    tsne = TSNE(perplexity=30, n_components=3, init='pca', n_iter=5000)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    X = tsne.fit_transform(X)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10), facecolor='white')\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # The problem is that the legend function don't support the type returned by a 3D scatter.\n",
    "    # So you have to create a \"dummy plot\" with the same characteristics and put those in the legend.\n",
    "    scatter_proxy = []\n",
    "    for i in range(C):\n",
    "        cls = classes_to_visual[i]\n",
    "        idx = np.array(labels) == cls\n",
    "        ax.scatter(X[idx, 0], X[idx, 1], X[idx, 2], c=colors[i], alpha=opaque[i] if opaque else 1, s=maker_size[i] if maker_size else 20, marker=markers[i], label=cls)\n",
    "        scatter_proxy.append(mpl.lines.Line2D([0],[0], linestyle=\"none\", c=colors[i], marker=markers[i], label=cls))\n",
    "    ax.legend(scatter_proxy, classes_to_visual, numpoints=1)\n",
    "    plt.savefig(save_file)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_pca_2d(doc_codes, doc_labels, classes_to_visual, save_file):\n",
    "    \"\"\"\n",
    "        Visualize the input data on a 2D PCA plot. Depending on the number of components,\n",
    "        the plot will contain an X amount of subplots.\n",
    "        @param doc_codes:\n",
    "        @param number_of_components: The number of principal components for the PCA plot.\n",
    "    \"\"\"\n",
    "    # markers = [\"D\", \"p\", \"*\", \"s\", \"d\", \"8\", \"^\", \"H\", \"v\", \">\", \"<\", \"h\", \"|\"]\n",
    "    markers = [\"o\", \"v\", \"8\", \"s\", \"p\", \"*\", \"h\", \"H\", \"+\", \"x\", \"D\"]\n",
    "    plt.rc('legend',**{'fontsize':28})\n",
    "    classes_to_visual = list(set(classes_to_visual))\n",
    "    C = len(classes_to_visual)\n",
    "    while True:\n",
    "        if C <= len(markers):\n",
    "            break\n",
    "        markers += markers\n",
    "\n",
    "    class_ids = dict(zip(classes_to_visual, range(C)))\n",
    "\n",
    "    if isinstance(doc_codes, dict) and isinstance(doc_labels, dict):\n",
    "        codes, labels = zip(*[(code, doc_labels[doc]) for doc, code in doc_codes.items() if doc_labels[doc] in classes_to_visual])\n",
    "    else:\n",
    "        codes, labels = doc_codes, doc_labels\n",
    "\n",
    "    X = np.r_[list(codes)]\n",
    "    X = PCA(n_components=3).fit_transform(X)\n",
    "    plt.figure(figsize=(10, 10), facecolor='white')\n",
    "\n",
    "    x_pc, y_pc = 1, 2\n",
    "\n",
    "    for c in classes_to_visual:\n",
    "        idx = np.array(labels) == c\n",
    "        # idx = get_indices(labels, c)\n",
    "        plt.plot(X[idx, x_pc], X[idx, y_pc], linestyle='None', alpha=1, marker=markers[class_ids[c]],\n",
    "                        markersize=10, label=c)\n",
    "        # plt.legend(c)\n",
    "    # plt.title('Projected on the PCA components')\n",
    "    # plt.xlabel('PC %s' % x_pc)\n",
    "    # plt.ylabel('PC %s' % y_pc)\n",
    "    legend = plt.legend(loc='upper right', shadow=True)\n",
    "    # plt.savefig(save_file)\n",
    "    plt.savefig(save_file, format='eps', dpi=2000)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_pca_3d(doc_codes, doc_labels, classes_to_visual, save_file, maker_size=None, opaque=None):\n",
    "    \"\"\"\n",
    "        Visualize the input data on a 2D PCA plot. Depending on the number of components,\n",
    "        the plot will contain an X amount of subplots.\n",
    "        @param doc_codes:\n",
    "        @param number_of_components: The number of principal components for the PCA plot.\n",
    "    \"\"\"\n",
    "    markers = [\"D\", \"p\", \"*\", \"s\", \"d\", \"8\", \"^\", \"H\", \"v\", \">\", \"<\", \"h\", \"|\"]\n",
    "    plt.rc('legend',**{'fontsize':20})\n",
    "    colors = ['r', 'b', 'g', 'c', 'm', 'y', 'k']\n",
    "    C = len(classes_to_visual)\n",
    "    while True:\n",
    "        if C <= len(markers):\n",
    "            break\n",
    "        markers += markers\n",
    "    while True:\n",
    "        if C <= len(colors):\n",
    "            break\n",
    "        colors += colors\n",
    "\n",
    "    if isinstance(doc_codes, dict) and isinstance(doc_labels, dict):\n",
    "        codes, labels = zip(*[(code, doc_labels[doc]) for doc, code in doc_codes.items() if doc_labels[doc] in classes_to_visual])\n",
    "    else:\n",
    "        codes, labels = doc_codes, doc_labels\n",
    "\n",
    "    X = np.r_[list(codes)]\n",
    "    X = PCA(n_components=3).fit_transform(X)\n",
    "    fig = plt.figure(figsize=(10, 10), facecolor='white')\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    x_pc, y_pc, z_pc = 0, 1, 2\n",
    "\n",
    "    # The problem is that the legend function don't support the type returned by a 3D scatter.\n",
    "    # So you have to create a \"dummy plot\" with the same characteristics and put those in the legend.\n",
    "    scatter_proxy = []\n",
    "    for i in range(C):\n",
    "        cls = classes_to_visual[i]\n",
    "        idx = np.array(labels) == cls\n",
    "        ax.scatter(X[idx, x_pc], X[idx, y_pc], X[idx, z_pc], c=colors[i], alpha=opaque[i] if opaque else 1, s=maker_size[i] if maker_size else 20, marker=markers[i], label=cls)\n",
    "        scatter_proxy.append(mpl.lines.Line2D([0],[0], linestyle=\"none\", c=colors[i], marker=markers[i], label=cls))\n",
    "    ax.legend(scatter_proxy, classes_to_visual, numpoints=1)\n",
    "    # plt.title('Projected on the PCA components')\n",
    "    ax.set_xlabel('%sst component' % (x_pc + 1), fontsize=14)\n",
    "    ax.set_ylabel('%snd component' % (y_pc + 1), fontsize=14)\n",
    "    ax.set_zlabel('%srd component' % (z_pc + 1), fontsize=14)\n",
    "    plt.savefig(save_file)\n",
    "    plt.show()\n",
    "\n",
    "def DBN_plot_tsne(doc_codes, doc_labels, classes_to_visual, save_file):\n",
    "    markers = [\"o\", \"v\", \"8\", \"s\", \"p\", \"*\", \"h\", \"H\", \"+\", \"x\", \"D\"]\n",
    "\n",
    "    C = len(classes_to_visual)\n",
    "    while True:\n",
    "        if C <= len(markers):\n",
    "            break\n",
    "        markers += markers\n",
    "\n",
    "    class_ids = dict(zip(classes_to_visual.keys(), range(C)))\n",
    "\n",
    "    codes, labels = doc_codes, doc_labels\n",
    "\n",
    "    X = np.r_[list(codes)]\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    X = tsne.fit_transform(X)\n",
    "\n",
    "    plt.figure(figsize=(10, 10), facecolor='white')\n",
    "\n",
    "    for c in classes_to_visual.keys():\n",
    "        idx = np.array(labels) == c\n",
    "        # idx = get_indices(labels, c)\n",
    "        plt.plot(X[idx, 0], X[idx, 1], linestyle='None', alpha=0.6, marker=markers[class_ids[c]],\n",
    "                        markersize=6, label=classes_to_visual[c])\n",
    "    legend = plt.legend(loc='upper center', shadow=True)\n",
    "    plt.title(\"tsne\")\n",
    "    plt.savefig(save_file)\n",
    "    plt.show()\n",
    "\n",
    "def DBN_visualize_pca_2d(doc_codes, doc_labels, classes_to_visual, save_file):\n",
    "    \"\"\"\n",
    "        Visualize the input data on a 2D PCA plot. Depending on the number of components,\n",
    "        the plot will contain an X amount of subplots.\n",
    "        @param doc_codes:\n",
    "        @param number_of_components: The number of principal components for the PCA plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # markers = [\"p\", \"s\", \"h\", \"H\", \"+\", \"x\", \"D\"]\n",
    "    markers = [\"o\", \"v\", \"8\", \"s\", \"p\", \"*\", \"h\", \"H\", \"+\", \"x\", \"D\"]\n",
    "\n",
    "    C = len(classes_to_visual)\n",
    "    while True:\n",
    "        if C <= len(markers):\n",
    "            break\n",
    "        markers += markers\n",
    "\n",
    "    class_ids = dict(zip(classes_to_visual.keys(), range(C)))\n",
    "\n",
    "    codes, labels = doc_codes, doc_labels\n",
    "\n",
    "    X = np.r_[list(codes)]\n",
    "    X = PCA(n_components=3).fit_transform(X)\n",
    "    plt.figure(figsize=(10, 10), facecolor='white')\n",
    "\n",
    "    x_pc, y_pc = 1, 2\n",
    "\n",
    "    for c in classes_to_visual.keys():\n",
    "        idx = np.array(labels) == c\n",
    "        # idx = get_indices(labels, c)\n",
    "        plt.plot(X[idx, x_pc], X[idx, y_pc], linestyle='None', alpha=0.6, marker=markers[class_ids[c]],\n",
    "                        markersize=6, label=classes_to_visual[c])\n",
    "        # plt.legend(c)\n",
    "    plt.title('Projected on the first 2 PCs')\n",
    "    plt.xlabel('PC %s' % x_pc)\n",
    "    plt.ylabel('PC %s' % y_pc)\n",
    "    # legend = plt.legend(loc='upper center', shadow=True)\n",
    "    plt.savefig(save_file)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def reuters_visualize_tsne(doc_codes, doc_labels, classes_to_visual, save_file):\n",
    "    \"\"\"\n",
    "        Visualize the input data on a 2D PCA plot. Depending on the number of components,\n",
    "        the plot will contain an X amount of subplots.\n",
    "        @param doc_codes:\n",
    "        @param number_of_components: The number of principal components for the PCA plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # markers = [\"p\", \"s\", \"h\", \"H\", \"+\", \"x\", \"D\"]\n",
    "    markers = [\"o\", \"v\", \"8\", \"s\", \"p\", \"*\", \"h\", \"H\", \"+\", \"x\", \"D\"]\n",
    "\n",
    "    C = len(classes_to_visual)\n",
    "    while True:\n",
    "        if C <= len(markers):\n",
    "            break\n",
    "        markers += markers\n",
    "\n",
    "    class_names = classes_to_visual.keys()\n",
    "    class_ids = dict(zip(class_names, range(C)))\n",
    "    class_names = set(class_names)\n",
    "    codes, labels = zip(*[(code, doc_labels[doc]) for doc, code in doc_codes.items() if class_names.intersection(set(doc_labels[doc]))])\n",
    "\n",
    "    X = np.r_[list(codes)]\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    X = tsne.fit_transform(X)\n",
    "\n",
    "    plt.figure(figsize=(10, 10), facecolor='white')\n",
    "\n",
    "    for c in classes_to_visual.keys():\n",
    "        idx = get_indices(labels, c)\n",
    "        plt.plot(X[idx, 0], X[idx, 1], linestyle='None', alpha=0.6, marker=markers[class_ids[c]],\n",
    "                        markersize=6, label=classes_to_visual[c])\n",
    "    legend = plt.legend(loc='upper center', shadow=True)\n",
    "    plt.title(\"tsne\")\n",
    "    plt.savefig(save_file)\n",
    "    plt.show()\n",
    "\n",
    "def reuters_visualize_pca_2d(doc_codes, doc_labels, classes_to_visual, save_file):\n",
    "    \"\"\"\n",
    "        Visualize the input data on a 2D PCA plot. Depending on the number of components,\n",
    "        the plot will contain an X amount of subplots.\n",
    "        @param doc_codes:\n",
    "        @param number_of_components: The number of principal components for the PCA plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # markers = [\"p\", \"s\", \"h\", \"H\", \"+\", \"x\", \"D\"]\n",
    "    markers = [\"o\", \"v\", \"8\", \"s\", \"p\", \"*\", \"h\", \"H\", \"+\", \"x\", \"D\"]\n",
    "\n",
    "    C = len(classes_to_visual)\n",
    "    while True:\n",
    "        if C <= len(markers):\n",
    "            break\n",
    "        markers += markers\n",
    "\n",
    "    class_names = classes_to_visual.keys()\n",
    "    class_ids = dict(zip(class_names, range(C)))\n",
    "    class_names = set(class_names)\n",
    "    codes, labels = zip(*[(code, class_names.intersection(set(doc_labels[doc]))) for doc, code in doc_codes.items() if len(class_names.intersection(set(doc_labels[doc]))) == 1])\n",
    "    # codes = []\n",
    "    # labels = []\n",
    "    # for doc, code in doc_codes.items():\n",
    "    #     y = set(doc_labels[doc])\n",
    "    #     x = list(class_names.intersection(y))\n",
    "    #     if x:\n",
    "    #         codes.append(code)\n",
    "    #         labels.append(x[0])\n",
    "    # x = 0\n",
    "    # pairs = []\n",
    "    # for each in labels:\n",
    "    #     if len(class_names.intersection(set(each))) > 1:\n",
    "    #         x += 1\n",
    "    #         pairs.append(class_names.intersection(set(each)))\n",
    "    # print x\n",
    "\n",
    "\n",
    "    X = np.r_[list(codes)]\n",
    "    X = PCA(n_components=3).fit_transform(X)\n",
    "    plt.figure(figsize=(10, 10), facecolor='white')\n",
    "\n",
    "    x_pc, y_pc = 0, 1\n",
    "\n",
    "    for c in class_names:\n",
    "        idx = get_indices(labels, c)\n",
    "        plt.plot(X[idx, x_pc], X[idx, y_pc], linestyle='None', alpha=0.6, marker=markers[class_ids[c]],\n",
    "                        markersize=6, label=classes_to_visual[c])\n",
    "        # plt.legend(c)\n",
    "    plt.title('Projected on the first 2 PCs')\n",
    "    plt.xlabel('PC %s' % x_pc)\n",
    "    plt.ylabel('PC %s' % y_pc)\n",
    "    legend = plt.legend(loc='upper center', shadow=True)\n",
    "    plt.savefig(save_file)\n",
    "    plt.show()\n",
    "\n",
    "def get_indices(labels, c):\n",
    "    idx = np.zeros(len(labels), dtype=bool)\n",
    "    for i in range(len(labels)):\n",
    "        tmp = [labels[i]] if not isinstance(labels[i], (list, set)) else labels[i]\n",
    "        if c in tmp:\n",
    "            idx[i] = True\n",
    "    return idx\n",
    "\n",
    "def plot_info_retrieval(precisions, save_file):\n",
    "    # markers = [\"|\", \"D\", \"8\", \"v\", \"^\", \">\", \"h\", \"H\", \"s\", \"*\", \"p\", \"d\", \"<\"]\n",
    "    markers = [\"D\", \"p\", 's', \"*\", \"d\", \"8\", \"^\", \"H\", \"v\", \">\", \"<\", \"h\", \"|\"]\n",
    "    #changing to list(zip) for Python3 compatibility SS*\n",
    "    ticks = list(zip(*list(zip(*precisions))[1][0]))[0]\n",
    "    plt.xticks(range(len(ticks)), ticks)\n",
    "    new_x = interpolate.interp1d(ticks, range(len(ticks)))(ticks)\n",
    "\n",
    "    i = 0\n",
    "    for model_name, val in precisions:\n",
    "        fr, pr = zip(*val)\n",
    "        plt.plot(new_x, pr, linestyle='-', alpha=0.7, marker=markers[i],\n",
    "                        markersize=8, label=model_name)\n",
    "        i += 1\n",
    "        # plt.legend(model_name)\n",
    "    plt.xlabel('Fraction of Retrieved Documents')\n",
    "    plt.ylabel('Precision')\n",
    "    legend = plt.legend(loc='upper right', shadow=True)\n",
    "    plt.savefig(save_file)\n",
    "    plt.show()\n",
    "\n",
    "def plot_info_retrieval_by_length(precisions, save_file):\n",
    "    markers = [\"o\", \"v\", \"8\", \"s\", \"p\", \"*\", \"h\", \"H\", \"^\", \"x\", \"D\"]\n",
    "    ticks = list(zip(*list(zip(*precisions))[1][0]))[0]\n",
    "    plt.xticks(range(len(ticks)), ticks)\n",
    "    new_x = interpolate.interp1d(ticks, range(len(ticks)))(ticks)\n",
    "\n",
    "    i = 0\n",
    "    for model_name, val in precisions:\n",
    "        fr, pr = zip(*val)\n",
    "        plt.plot(new_x, pr, linestyle='-', alpha=0.6, marker=markers[i],\n",
    "                        markersize=6, label=model_name)\n",
    "        i += 1\n",
    "        # plt.legend(model_name)\n",
    "    plt.xlabel('Document Sorted by Length')\n",
    "    plt.ylabel('Precision (%)')\n",
    "    legend = plt.legend(loc='upper right', shadow=True)\n",
    "    plt.savefig(save_file)\n",
    "    plt.show()\n",
    "\n",
    "def plot(x, y, x_label, y_label, save_file):\n",
    "    ticks = x\n",
    "    plt.xticks(range(len(ticks)), ticks, fontsize = 15)\n",
    "    plt.yticks(fontsize = 15)\n",
    "    new_x = interpolate.interp1d(ticks, range(len(ticks)))(ticks)\n",
    "\n",
    "    plt.plot(new_x, y, linestyle='-', alpha=1.0, markersize=12, marker='p', color='b')\n",
    "    plt.xlabel(x_label, fontsize=24)\n",
    "    plt.ylabel(y_label, fontsize=20)\n",
    "    plt.savefig(save_file)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    # 20news_retrieval_128D\n",
    "    precisions = [\n",
    "        ('VAE', [(0.001, 0.587348525080869), (0.002, 0.5651402500844888), (0.005, 0.5327151771489245), (0.01, 0.5014839340348453), (0.02, 0.4584269359288251), (0.05, 0.3658133556412997), (0.1, 0.2687164883998648), (0.2, 0.17739560251738207), (0.5, 0.09136516909151776), (1.0, 0.05050301672031405)]),\n",
    "        ('DocNADE', [(0.001, 0.5718148022980761), (0.002, 0.5435414956790445), (0.005, 0.5074230900538642), (0.01, 0.4746133312027964), (0.02, 0.43102761550716634), (0.05, 0.3383512940656766), (0.1, 0.25088957318799715), (0.2, 0.16893256617330504), (0.5, 0.0898931631614369), (1.0, 0.05050301672031405)]),\n",
    "        ('KATE', [(0.001, 0.5543982040264583), (0.002, 0.5213392555399969), (0.005, 0.4739445034519384), (0.01, 0.4347574243698827), (0.02, 0.3869114198299623), (0.05, 0.30403564261511706), (0.1, 0.2277761656366975), (0.2, 0.15699569840064684), (0.5, 0.08683891514289452), (1.0, 0.05050301672031405)]),\n",
    "        ('DBN', [(0.001, 0.535038381692656), (0.002, 0.5077608265340592), (0.005, 0.465912108337758), (0.01, 0.4264154357337848), (0.02, 0.37657322856108594), (0.05, 0.29198182151435376), (0.1, 0.2197600288870639), (0.2, 0.15325609847145583), (0.5, 0.08605947016611057), (1.0, 0.05050301672031403)]),\n",
    "        ('LDA',  [(0.001, 0.4867957321488915), (0.002, 0.46359774054941044), (0.005, 0.42999155982095444), (0.01, 0.40179481997753264), (0.02, 0.36320959775165296), (0.05, 0.2823678558504475), (0.1, 0.20784423242441585), (0.2, 0.14168207983103592), (0.5, 0.0800605531419018), (1.0, 0.05050301672031405)]),\n",
    "        ('Word2Vec_pre', [(0.001, 0.4619200502100128), (0.002, 0.4201226283010617), (0.005, 0.363601016614824), (0.01, 0.3199258385461033), (0.02, 0.27425227583548745), (0.05, 0.2083981501934101), (0.1, 0.15890115524777892), (0.2, 0.1170970848576276), (0.5, 0.0746015280886044), (1.0, 0.05050301672031405)]),\n",
    "        ('Word2Vec', [(0.001, 0.3815960990682146), (0.002, 0.33990126973397794), (0.005, 0.28322016538957506), (0.01, 0.23994026666166862), (0.02, 0.1996232005978027), (0.05, 0.15086380704863955), (0.1, 0.11886672273161164), (0.2, 0.09250234660438485), (0.5, 0.06552700581695815), (1.0, 0.05050301672031405)]),\n",
    "        ('CAE',  [(0.001, 0.25605899676530824), (0.002, 0.2178643846859439), (0.005, 0.17500331917153453), (0.01, 0.14827356083073284), (0.02, 0.12567969583465557), (0.05, 0.1013255537435644), (0.1, 0.08658477146491544), (0.2, 0.07424743141317958), (0.5, 0.059710587487142405), (1.0, 0.05050301672031405)]),\n",
    "        ('KSAE', [(0.001, 0.23964418481146235), (0.002, 0.20264447448462075), (0.005, 0.16342178135194532), (0.01, 0.1395696943777457), (0.02, 0.12070563824438621), (0.05, 0.09967618984957147), (0.1, 0.08657995851945419), (0.2, 0.07516400405132624), (0.5, 0.061121338068411066), (1.0, 0.05050301672031405)]),\n",
    "        ('AE', [(0.001, 0.22827451359049142), (0.002, 0.18935571863080838), (0.005, 0.14794495865260598), (0.01, 0.12336861250406352), (0.02, 0.10404868431566056), (0.05, 0.08489066120247557), (0.1, 0.07413015988839661), (0.2, 0.06568426232571814), (0.5, 0.0563362391994616), (1.0, 0.05050301672031405)]),\n",
    "        ('DAE', [(0.001, 0.2095785255636476), (0.002, 0.17031574373581437), (0.005, 0.1300285448751998), (0.01, 0.10855864535504914), (0.02, 0.09279581161675608), (0.05, 0.07767683840981233), (0.1, 0.06946348101328252), (0.2, 0.06284691358720304), (0.5, 0.05536974244871757), (1.0, 0.05050301672031405)]),\n",
    "        ('Doc2Vec', [(0.001, 0.16486023270409583), (0.002, 0.1494834162120367), (0.005, 0.12679472346559542), (0.01, 0.11052195000447207), (0.02, 0.0953665540302444), (0.05, 0.07877845088096704), (0.1, 0.06914265711214816), (0.2, 0.06190158066520009), (0.5, 0.05536713733618202), (1.0, 0.05050301672031404)]),\n",
    "        ('NVDM', [(0.001, 0.05129628735576586), (0.002, 0.0513143919277744), (0.005, 0.0513784045216623), (0.01, 0.05057947447821584), (0.02, 0.04999729766565648), (0.05, 0.05015274063700316), (0.1, 0.050297158296132634), (0.2, 0.05053768818029844), (0.5, 0.050492220758456205), (1.0, 0.05050301672031404)])\n",
    "        ]\n",
    "\n",
    "    # 20news_retrieval_512D\n",
    "    # precisions = {\n",
    "    # 'LDA':  [(0.001, 0.4058682952734931), (0.002, 0.37058851928739733), (0.005, 0.3309972687959938), (0.01, 0.3016110612419513), (0.02, 0.2693117036925588), (0.05, 0.2161839279252353), (0.1, 0.1702690976502034), (0.2, 0.12488871530981528), (0.5, 0.07553462776603063), (1.0, 0.05050301672031405)],\n",
    "    # 'DBN': [(0.001, 0.5553034326268522), (0.002, 0.5285147009124683), (0.005, 0.488347337076094), (0.01, 0.45024297510562405), (0.02, 0.4033891972422051), (0.05, 0.31771321417997606), (0.1, 0.23800250085341837), (0.2, 0.1643866217959289), (0.5, 0.08997211449990615), (1.0, 0.050503016720313966)],\n",
    "    # 'DocNADE': [(0.001, 0.5771737556124188), (0.002, 0.5443682711340714), (0.005, 0.5036226386465347), (0.01, 0.47000408874935945), (0.02, 0.42806973432528334), (0.05, 0.3391154672218619), (0.1, 0.2523257091581672), (0.2, 0.16856842576301645), (0.5, 0.08886419064880063), (1.0, 0.05050301672031405)],\n",
    "    # 'NVDM':  [(0.001, 0.051827354801331486), (0.002, 0.05166441365326164), (0.005, 0.05022143615810757), (0.01, 0.050504279087694), (0.02, 0.04984220717270456), (0.05, 0.050210312107871934), (0.1, 0.05031805352276878), (0.2, 0.050525479733275175), (0.5, 0.05048740951458409), (1.0, 0.05050301672031404)],\n",
    "    # 'Word2Vec_pre': [(0.001, 0.4619200502100128), (0.002, 0.4201226283010617), (0.005, 0.363601016614824), (0.01, 0.3199258385461033), (0.02, 0.27425227583548745), (0.05, 0.2083981501934101), (0.1, 0.15890115524777892), (0.2, 0.1170970848576276), (0.5, 0.0746015280886044), (1.0, 0.05050301672031405)],\n",
    "    # 'Word2Vec': [(0.001, 0.3842755757253872), (0.002, 0.34131946120793055), (0.005, 0.28388162885972246), (0.01, 0.24101532576054588), (0.02, 0.20010492106833672), (0.05, 0.1509728403648974), (0.1, 0.118880457234514), (0.2, 0.09250264007666884), (0.5, 0.0655253160142321), (1.0, 0.05050301672031405)],\n",
    "    # 'Doc2Vec':  [(0.001, 0.2199705498961938), (0.002, 0.19429826678896794), (0.005, 0.15887688718610055), (0.01, 0.13127352793274671), (0.02, 0.1067768670780567), (0.05, 0.08213522011101435), (0.1, 0.06901493797404573), (0.2, 0.05975776562880724), (0.5, 0.05340344575184013), (1.0, 0.050503016720314126)],\n",
    "    # 'AE':  [(0.001, 0.25062762516293363), (0.002, 0.2055713802925667), (0.005, 0.15574975343297148), (0.01, 0.12843020222861343), (0.02, 0.10797588107849927), (0.05, 0.08867698410088286), (0.1, 0.07749710871105607), (0.2, 0.06824739056183722), (0.5, 0.057956525318736705), (1.0, 0.05050301672031405)],\n",
    "    # 'DAE': [(0.001, 0.2441703278134424), (0.002, 0.19592767826968308), (0.005, 0.14740440785979805), (0.01, 0.12183063178228161), (0.02, 0.1024848551783867), (0.05, 0.08429144793424902), (0.1, 0.07442198872784735), (0.2, 0.06632051023795682), (0.5, 0.05714302612312982), (1.0, 0.05050301672031405)],\n",
    "    # 'CAE':  [(0.001, 0.2564210882054672), (0.002, 0.20924660841017487), (0.005, 0.16076881496092835), (0.01, 0.1325765230591466), (0.02, 0.11132618820467256), (0.05, 0.09089712800606133), (0.1, 0.07922084751978385), (0.2, 0.06933241629113954), (0.5, 0.058349474860945535), (1.0, 0.05050301672031405)],\n",
    "    # 'VAE':  [(0.001, 0.2864384685945949), (0.002, 0.22214913339448517), (0.005, 0.15730739321751025), (0.01, 0.12415463932062157), (0.02, 0.10121123325141194), (0.05, 0.08010235972535741), (0.1, 0.06876865603310822), (0.2, 0.06069940080002707), (0.5, 0.05329987492643488), (1.0, 0.05050301672031405)],\n",
    "    # 'KSAE': [(0.001, 0.2766257905663044), (0.002, 0.23146091826389079), (0.005, 0.18327753964039104), (0.01, 0.15379102261032626), (0.02, 0.13065375342492558), (0.05, 0.10653729926356474), (0.1, 0.09147214149778002), (0.2, 0.07821235936221227), (0.5, 0.0621660351341905), (1.0, 0.05050301672031405)],\n",
    "    # 'KATE': [(0.001, 0.5370057451841842), (0.002, 0.49623424902234925), (0.005, 0.4398091950534882), (0.01, 0.39517410082761617), (0.02, 0.3440230238886337), (0.05, 0.26452986431933806), (0.1, 0.19919513465212774), (0.2, 0.14045712651660616), (0.5, 0.08193839335997657), (1.0, 0.05050301672031405)]}\n",
    "\n",
    "    # precisions = {\n",
    "    # 'DocNADE': [(100, 0.5620457973399164), (120, 0.6721578198088268), (150, 0.6984651711924437), (200, 0.6809496236247824), (300, 0.518887505188875), (1000, 0.3119956966110817), (1500, 0.1818181818181818), (2000, 0.13636363636363635), (4000, 0.03305785123966942)],\n",
    "    # 'KCAE': [(100, 0.517573929338634), (120, 0.6815131177547284), (150, 0.7079102715466347), (200, 0.7348002316155173), (300, 0.6832710668327107), (1000, 0.6503496503496502), (1500, 0.6969696969696969), (2000, 0.8522727272727273), (4000, 0.42975206611570244)],\n",
    "\n",
    "    # }\n",
    "\n",
    "\n",
    "    # plot_info_retrieval_by_length(precisions, sys.argv[1])\n",
    "    plot_info_retrieval(precisions, sys.argv[1])\n",
    "\n",
    "    # # Effect of number of topics\n",
    "    # x = [20, 32, 64, 128, 256, 512, 1024, 1500]\n",
    "    # y = [0.546, 0.694, 0.719, 0.744, 0.747, 0.761, 0.767, 0.713]\n",
    "    # plot(x, y, 'Number of topics', 'Classification accuracy', sys.argv[1])\n",
    "\n",
    "    # Effect of alpha\n",
    "    # x = [0.0625, 0.3, 1, 3, 6, 9, 12]\n",
    "    # y = [0.711, 0.706, 0.739, 0.738, 0.743, 0.746, 0.743]\n",
    "    # plot(x, y, r'$\\alpha$', 'Classification accuracy', sys.argv[1])\n",
    "\n",
    "    # # # Effect of k\n",
    "    # x = [2, 4, 6, 8, 16, 32, 64, 96, 128]\n",
    "    # y = [0.729, 0.728, 0.720, 0.738, 0.737,  0.744, 0.739, 0.733, 0.714]\n",
    "    # plot(x, y, r'$k$', 'Classification accuracy', sys.argv[1])\n",
    "\n",
    "    # # scalability\n",
    "    # x = [0.5, 1, 1.5, 2]\n",
    "    # y = [6 , 12.3, 15.8,  49.5]\n",
    "    # plot(x, y, r'training set size', 'runtime (h)', sys.argv[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Replace from keras_utils import Dense_tied, KCompetitive, contractive_loss, CustomModelCheckpoint\n",
    "\n",
    "'''\n",
    "Created on Nov, 2016\n",
    "\n",
    "@author: hugo\n",
    "\n",
    "'''\n",
    "from __future__ import absolute_import\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import Callback\n",
    "import keras.backend as K\n",
    "from keras.engine import Layer\n",
    "import tensorflow as tf\n",
    "from keras import initializers\n",
    "import warnings\n",
    "\n",
    "#from ..testing.visualize import heatmap\n",
    "#rom .op_utils import unitmatrix replace cell \n",
    "\n",
    "def contractive_loss(model, lam=1e-4):\n",
    "    def loss(y_true, y_pred):\n",
    "        ent_loss = K.mean(K.binary_crossentropy(y_pred, y_true), axis=-1)\n",
    "\n",
    "        W = K.variable(value=model.encoder.get_weights()[0])  # N x N_hidden\n",
    "        W = K.transpose(W)  # N_hidden x N\n",
    "        h = model.encoder.output\n",
    "        dh = h * (1 - h)  # N_batch x N_hidden\n",
    "\n",
    "        # N_batch x N_hidden * N_hidden x 1 = N_batch x 1\n",
    "        contractive = lam * K.sum(dh**2 * K.sum(W**2, axis=1), axis=1)\n",
    "\n",
    "        return ent_loss + contractive\n",
    "    return loss\n",
    "\n",
    "\n",
    "def weighted_binary_crossentropy(feature_weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        # try:\n",
    "        #     x = K.binary_crossentropy(y_pred, y_true)\n",
    "        #     # y = tf.Variable(feature_weights.astype('float32'))\n",
    "        #     # z = K.dot(x, y)\n",
    "        #     y_true = tf.pow(y_true + 1e-5, .75)\n",
    "        #     y2 = tf.div(y_true, tf.reshape(K.sum(y_true, 1), [-1, 1]))\n",
    "        #     z = K.sum(tf.mul(x, y2), 1)\n",
    "        # except Exception as e:\n",
    "        #     print e\n",
    "        #     import pdb;pdb.set_trace()\n",
    "        # return z\n",
    "        return K.dot(K.binary_crossentropy(y_pred, y_true), K.variable(feature_weights.astype('float32')))\n",
    "    return loss\n",
    "\n",
    "\n",
    "class KCompetitive(Layer):\n",
    "    '''Applies K-Competitive layer.\n",
    "\n",
    "    # Arguments\n",
    "    '''\n",
    "    def __init__(self, topk, ctype, **kwargs):\n",
    "        self.topk = topk\n",
    "        self.ctype = ctype\n",
    "        self.uses_learning_phase = True\n",
    "        self.supports_masking = True\n",
    "        super(KCompetitive, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        if self.ctype == 'ksparse':\n",
    "            return K.in_train_phase(self.kSparse(x, self.topk), x)\n",
    "        elif self.ctype == 'kcomp':\n",
    "            return K.in_train_phase(self.k_comp_tanh(x, self.topk), x)\n",
    "        else:\n",
    "            warnings.warn(\"Unknown ctype, using no competition.\")\n",
    "            return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'topk': self.topk, 'ctype': self.ctype}\n",
    "        base_config = super(KCompetitive, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    # def k_comp_sigm(self, x, topk):\n",
    "    #     print 'run k_comp_sigm'\n",
    "    #     dim = int(x.get_shape()[1])\n",
    "    #     if topk > dim:\n",
    "    #         warnings.warn('topk should not be larger than dim: %s, found: %s, using %s' % (dim, topk, dim))\n",
    "    #         topk = dim\n",
    "\n",
    "    #     values, indices = tf.nn.top_k(x, topk) # indices will be [[0, 1], [2, 1]], values will be [[6., 2.], [5., 4.]]\n",
    "\n",
    "    #     # We need to create full indices like [[0, 0], [0, 1], [1, 2], [1, 1]]\n",
    "    #     my_range = tf.expand_dims(tf.range(0, K.shape(indices)[0]), 1)  # will be [[0], [1]]\n",
    "    #     my_range_repeated = tf.tile(my_range, [1, topk])  # will be [[0, 0], [1, 1]]\n",
    "\n",
    "    #     full_indices = tf.stack([my_range_repeated, indices], axis=2) # change shapes to [N, k, 1] and [N, k, 1], to concatenate into [N, k, 2]\n",
    "    #     full_indices = tf.reshape(full_indices, [-1, 2])\n",
    "\n",
    "    #     to_reset = tf.sparse_to_dense(full_indices, tf.shape(x), tf.reshape(values, [-1]), default_value=0., validate_indices=False)\n",
    "\n",
    "    #     batch_size = tf.to_float(tf.shape(x)[0])\n",
    "    #     tmp = 1 * batch_size * tf.reduce_sum(x - to_reset, 1, keep_dims=True) / topk\n",
    "\n",
    "    #     res = tf.sparse_to_dense(full_indices, tf.shape(x), tf.reshape(tf.add(values, tmp), [-1]), default_value=0., validate_indices=False)\n",
    "\n",
    "    #     return res\n",
    "\n",
    "    def k_comp_tanh(self, x, topk, factor=6.26):\n",
    "        print('run k_comp_tanh')\n",
    "        dim = int(x.get_shape()[1])\n",
    "        # batch_size = tf.to_float(tf.shape(x)[0])\n",
    "        if topk > dim:\n",
    "            warnings.warn('Warning: topk should not be larger than dim: %s, found: %s, using %s' % (dim, topk, dim))\n",
    "            topk = dim\n",
    "\n",
    "        P = (x + tf.abs(x)) / 2\n",
    "        N = (x - tf.abs(x)) / 2\n",
    "\n",
    "        values, indices = tf.nn.top_k(P, topk / 2) # indices will be [[0, 1], [2, 1]], values will be [[6., 2.], [5., 4.]]\n",
    "        # We need to create full indices like [[0, 0], [0, 1], [1, 2], [1, 1]]\n",
    "        my_range = tf.expand_dims(tf.range(0, tf.shape(indices)[0]), 1)  # will be [[0], [1]]\n",
    "        my_range_repeated = tf.tile(my_range, [1, topk / 2])  # will be [[0, 0], [1, 1]]\n",
    "        full_indices = tf.stack([my_range_repeated, indices], axis=2) # change shapes to [N, k, 1] and [N, k, 1], to concatenate into [N, k, 2]\n",
    "        full_indices = tf.reshape(full_indices, [-1, 2])\n",
    "        P_reset = tf.sparse_to_dense(full_indices, tf.shape(x), tf.reshape(values, [-1]), default_value=0., validate_indices=False)\n",
    "\n",
    "\n",
    "        values2, indices2 = tf.nn.top_k(-N, topk - topk / 2)\n",
    "        my_range = tf.expand_dims(tf.range(0, tf.shape(indices2)[0]), 1)\n",
    "        my_range_repeated = tf.tile(my_range, [1, topk - topk / 2])\n",
    "        full_indices2 = tf.stack([my_range_repeated, indices2], axis=2)\n",
    "        full_indices2 = tf.reshape(full_indices2, [-1, 2])\n",
    "        N_reset = tf.sparse_to_dense(full_indices2, tf.shape(x), tf.reshape(values2, [-1]), default_value=0., validate_indices=False)\n",
    "\n",
    "\n",
    "        # 1)\n",
    "        # res = P_reset - N_reset\n",
    "        # tmp = 1 * batch_size * tf.reduce_sum(x - res, 1, keep_dims=True) / topk\n",
    "\n",
    "        # P_reset = tf.sparse_to_dense(full_indices, tf.shape(x), tf.reshape(tf.add(values, tf.abs(tmp)), [-1]), default_value=0., validate_indices=False)\n",
    "        # N_reset = tf.sparse_to_dense(full_indices2, tf.shape(x), tf.reshape(tf.add(values2, tf.abs(tmp)), [-1]), default_value=0., validate_indices=False)\n",
    "\n",
    "        # 2)\n",
    "        # factor = 0.\n",
    "        # factor = 2. / topk\n",
    "        P_tmp = factor * tf.reduce_sum(P - P_reset, 1, keep_dims=True) # 6.26\n",
    "        N_tmp = factor * tf.reduce_sum(-N - N_reset, 1, keep_dims=True)\n",
    "        P_reset = tf.sparse_to_dense(full_indices, tf.shape(x), tf.reshape(tf.add(values, P_tmp), [-1]), default_value=0., validate_indices=False)\n",
    "        N_reset = tf.sparse_to_dense(full_indices2, tf.shape(x), tf.reshape(tf.add(values2, N_tmp), [-1]), default_value=0., validate_indices=False)\n",
    "\n",
    "        res = P_reset - N_reset\n",
    "\n",
    "        return res\n",
    "\n",
    "    # def k_comp_tanh_strict(self, x, topk):\n",
    "    #     print 'run k_comp_tanh_strict'\n",
    "    #     dim = int(x.get_shape()[1])\n",
    "    #     # batch_size = tf.to_float(tf.shape(x)[0])\n",
    "    #     if topk > dim:\n",
    "    #         warnings.warn('Warning: topk should not be larger than dim: %s, found: %s, using %s' % (dim, topk, dim))\n",
    "    #         topk = dim\n",
    "\n",
    "    #     x_abs = tf.abs(x)\n",
    "    #     P = (x + x_abs) / 2 # positive part of x\n",
    "    #     N = (x - x_abs) / 2 # negative part of x\n",
    "\n",
    "    #     values, indices = tf.nn.top_k(x_abs, topk) # indices will be [[0, 1], [2, 1]], values will be [[6., 2.], [5., 4.]]\n",
    "    #     # We need to create full indices like [[0, 0], [0, 1], [1, 2], [1, 1]]\n",
    "    #     my_range = tf.expand_dims(tf.range(0, tf.shape(indices)[0]), 1)  # will be [[0], [1]]\n",
    "    #     my_range_repeated = tf.tile(my_range, [1, topk])  # will be [[0, 0], [1, 1]]\n",
    "    #     full_indices = tf.stack([my_range_repeated, indices], axis=2) # change shapes to [N, k, 1] and [N, k, 1], to concatenate into [N, k, 2]\n",
    "    #     full_indices = tf.reshape(full_indices, [-1, 2])\n",
    "    #     x_topk_mask = tf.sparse_to_dense(full_indices, tf.shape(x), tf.ones([tf.shape(full_indices)[0], ], tf.float32), default_value=0., validate_indices=False)\n",
    "\n",
    "    #     P_select = tf.multiply(P, x_topk_mask)\n",
    "    #     N_select = tf.multiply(-N, x_topk_mask)\n",
    "\n",
    "    #     zero = tf.constant(0., dtype=tf.float32)\n",
    "    #     P_indices = tf.cast(tf.where(tf.not_equal(P_select, zero)), tf.int32)\n",
    "    #     N_indices = tf.cast(tf.where(tf.not_equal(N_select, zero)), tf.int32)\n",
    "    #     P_mask = tf.sparse_to_dense(P_indices, tf.shape(x), tf.ones([tf.shape(P_indices)[0], ], tf.float32), default_value=0., validate_indices=False)\n",
    "    #     N_mask = tf.sparse_to_dense(N_indices, tf.shape(x), tf.ones([tf.shape(N_indices)[0], ], tf.float32), default_value=0., validate_indices=False)\n",
    "\n",
    "    #     alpha = 10.\n",
    "    #     P_complement = alpha * tf.reduce_sum(P - P_select, 1, keep_dims=True)# / tf.cast(tf.shape(P_indices)[0], tf.float32) # 6.26\n",
    "    #     N_complement = alpha * tf.reduce_sum(-N - N_select, 1, keep_dims=True)# / tf.cast(tf.shape(N_indices)[0], tf.float32)\n",
    "\n",
    "    #     P_reset = tf.multiply(tf.add(P_select, P_complement), P_mask)\n",
    "    #     N_reset = tf.multiply(tf.add(N_select, N_complement), N_mask)\n",
    "    #     res = P_reset - N_reset\n",
    "\n",
    "    #     return res\n",
    "\n",
    "    def kSparse(self, x, topk):\n",
    "        print('run regular k-sparse')\n",
    "        dim = int(x.get_shape()[1])\n",
    "        if topk > dim:\n",
    "            warnings.warn('Warning: topk should not be larger than dim: %s, found: %s, using %s' % (dim, topk, dim))\n",
    "            topk = dim\n",
    "\n",
    "        k = dim - topk\n",
    "        values, indices = tf.nn.top_k(-x, k) # indices will be [[0, 1], [2, 1]], values will be [[6., 2.], [5., 4.]]\n",
    "\n",
    "        # We need to create full indices like [[0, 0], [0, 1], [1, 2], [1, 1]]\n",
    "        my_range = tf.expand_dims(tf.range(0, tf.shape(indices)[0]), 1)  # will be [[0], [1]]\n",
    "        my_range_repeated = tf.tile(my_range, [1, k])  # will be [[0, 0], [1, 1]]\n",
    "\n",
    "        full_indices = tf.stack([my_range_repeated, indices], axis=2) # change shapes to [N, k, 1] and [N, k, 1], to concatenate into [N, k, 2]\n",
    "        full_indices = tf.reshape(full_indices, [-1, 2])\n",
    "\n",
    "        to_reset = tf.sparse_to_dense(full_indices, tf.shape(x), tf.reshape(values, [-1]), default_value=0., validate_indices=False)\n",
    "\n",
    "        res = tf.add(x, to_reset)\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "class Dense_tied(Dense):\n",
    "    \"\"\"\n",
    "    A fully connected layer with tied weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, units,\n",
    "                 activation=None, use_bias=True,\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None, bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None, bias_constraint=None,\n",
    "                 tied_to=None, **kwargs):\n",
    "        self.tied_to = tied_to\n",
    "\n",
    "        super(Dense_tied, self).__init__(units=units,\n",
    "                 activation=activation, use_bias=use_bias,\n",
    "                 bias_initializer=bias_initializer,\n",
    "                 kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer,\n",
    "                 activity_regularizer=activity_regularizer,\n",
    "                 kernel_constraint=kernel_constraint, bias_constraint=bias_constraint,\n",
    "                 **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Dense_tied, self).build(input_shape)  # be sure you call this somewhere!\n",
    "        if self.kernel in self.trainable_weights:\n",
    "            self.trainable_weights.remove(self.kernel)\n",
    "\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # Use tied weights\n",
    "        self.kernel = K.transpose(self.tied_to.kernel)\n",
    "        output = K.dot(x, self.kernel)\n",
    "        if self.use_bias:\n",
    "            output += self.bias\n",
    "        return self.activation(output)\n",
    "\n",
    "class CustomModelCheckpoint(Callback):\n",
    "    \"\"\"Save the model after every epoch.\n",
    "    `filepath` can contain named formatting options,\n",
    "    which will be filled the value of `epoch` and\n",
    "    keys in `logs` (passed in `on_epoch_end`).\n",
    "    For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`,\n",
    "    then the model checkpoints will be saved with the epoch number and\n",
    "    the validation loss in the filename.\n",
    "    # Arguments\n",
    "        filepath: string, path to save the model file.\n",
    "        monitor: quantity to monitor.\n",
    "        verbose: verbosity mode, 0 or 1.\n",
    "        save_best_only: if `save_best_only=True`,\n",
    "            the latest best model according to\n",
    "            the quantity monitored will not be overwritten.\n",
    "        mode: one of {auto, min, max}.\n",
    "            If `save_best_only=True`, the decision\n",
    "            to overwrite the current save file is made\n",
    "            based on either the maximization or the\n",
    "            minimization of the monitored quantity. For `val_acc`,\n",
    "            this should be `max`, for `val_loss` this should\n",
    "            be `min`, etc. In `auto` mode, the direction is\n",
    "            automatically inferred from the name of the monitored quantity.\n",
    "        save_weights_only: if True, then only the model's weights will be\n",
    "            saved (`model.save_weights(filepath)`), else the full model\n",
    "            is saved (`model.save(filepath)`).\n",
    "        period: Interval (number of epochs) between checkpoints.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, custom_model, filepath, monitor='val_loss', verbose=0,\n",
    "                 save_best_only=False, save_weights_only=False,\n",
    "                 mode='auto', period=1):\n",
    "        super(CustomModelCheckpoint, self).__init__()\n",
    "        self.custom_model = custom_model\n",
    "        self.monitor = monitor\n",
    "        self.verbose = verbose\n",
    "        self.filepath = filepath\n",
    "        self.save_best_only = save_best_only\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.period = period\n",
    "        self.epochs_since_last_save = 0\n",
    "\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('CustomModelCheckpoint mode %s is unknown, '\n",
    "                          'fallback to auto mode.' % (mode),\n",
    "                          RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
    "                self.monitor_op = np.greater\n",
    "                self.best = -np.Inf\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "                self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        model = self.custom_model\n",
    "        logs = logs or {}\n",
    "        self.epochs_since_last_save += 1\n",
    "        if self.epochs_since_last_save >= self.period:\n",
    "            self.epochs_since_last_save = 0\n",
    "            filepath = self.filepath.format(epoch=epoch, **logs)\n",
    "            if self.save_best_only:\n",
    "                current = logs.get(self.monitor)\n",
    "                if current is None:\n",
    "                    warnings.warn('Can save best model only with %s available, '\n",
    "                                  'skipping.' % (self.monitor), RuntimeWarning)\n",
    "                else:\n",
    "                    if self.monitor_op(current, self.best):\n",
    "                        if self.verbose > 0:\n",
    "                            print('Epoch %05d: %s improved from %0.5f to %0.5f,'\n",
    "                                  ' saving model to %s'\n",
    "                                  % (epoch, self.monitor, self.best,\n",
    "                                     current, filepath))\n",
    "                        self.best = current\n",
    "                        if self.save_weights_only:\n",
    "                            model.save_weights(filepath, overwrite=True)\n",
    "                        else:\n",
    "                            model.save(filepath, overwrite=True)\n",
    "                    else:\n",
    "                        if self.verbose > 0:\n",
    "                            print('Epoch %05d: %s did not improve' %\n",
    "                                  (epoch, self.monitor))\n",
    "            else:\n",
    "                if self.verbose > 0:\n",
    "                    print('Epoch %05d: saving model to %s' % (epoch, filepath))\n",
    "                if self.save_weights_only:\n",
    "                    model.save_weights(filepath, overwrite=True)\n",
    "                else:\n",
    "                    model.save(filepath, overwrite=True)\n",
    "\n",
    "class VisualWeights(Callback):\n",
    "    def __init__(self, save_path, per_epoch=15):\n",
    "        super(VisualWeights, self).__init__()\n",
    "        self.per_epoch = per_epoch\n",
    "        self.filename, self.ext = os.path.splitext(save_path)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Called at the end of an epoch.\n",
    "        # Arguments\n",
    "            epoch: integer, index of epoch.\n",
    "            logs: dictionary of logs.\n",
    "        \"\"\"\n",
    "        if epoch % self.per_epoch == 0:\n",
    "            weights = self.model.get_weights()[0]\n",
    "            # weights /= np.max(np.abs(weights))\n",
    "            weights = unitmatrix(weights, axis=0) # normalize\n",
    "            # weights[np.abs(weights) < 1e-2] = 0\n",
    "            heatmap(weights.T, '%s_%s%s'%(self.filename, epoch, self.ext))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Autoencoder.core.ae \n",
    "'''\n",
    "Created on Nov, 2016\n",
    "\n",
    "@author: hugo\n",
    "\n",
    "'''\n",
    "from __future__ import absolute_import\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.models import load_model as load_keras_model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "#from keras_utils import Dense_tied, KCompetitive, contractive_loss, CustomModelCheckpoint\n",
    "\n",
    "\n",
    "class AutoEncoder(object):\n",
    "    \"\"\"AutoEncoder for topic modeling.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, dim, comp_topk=None, ctype=None, save_model='best_model'):\n",
    "        self.input_size = input_size\n",
    "        self.dim = dim\n",
    "        self.comp_topk = comp_topk\n",
    "        self.ctype = ctype\n",
    "        self.save_model = save_model\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        # this is our input placeholder\n",
    "        input_layer = Input(shape=(self.input_size,))\n",
    "\n",
    "        # \"encoded\" is the encoded representation of the input\n",
    "        if self.ctype == None:\n",
    "            act = 'sigmoid'\n",
    "        elif self.ctype == 'kcomp':\n",
    "            act = 'tanh'\n",
    "        elif self.ctype == 'ksparse':\n",
    "            act = 'linear'\n",
    "        else:\n",
    "            raise Exception('unknown ctype: %s' % self.ctype)\n",
    "        encoded_layer = Dense(self.dim, activation=act, kernel_initializer=\"glorot_normal\", name=\"Encoded_Layer\")\n",
    "        encoded = encoded_layer(input_layer)\n",
    "\n",
    "        if self.comp_topk:\n",
    "            print('add k-competitive layer')\n",
    "            encoded = KCompetitive(self.comp_topk, self.ctype)(encoded)\n",
    "\n",
    "        # \"decoded\" is the lossy reconstruction of the input\n",
    "        # add non-negativity contraint to ensure probabilistic interpretations\n",
    "        decoded = Dense_tied(self.input_size, activation='sigmoid', tied_to=encoded_layer, name='Decoded_Layer')(encoded)\n",
    "\n",
    "        # this model maps an input to its reconstruction\n",
    "        self.autoencoder = Model(outputs=decoded, inputs=input_layer)\n",
    "\n",
    "        # this model maps an input to its encoded representation\n",
    "        self.encoder = Model(outputs=encoded, inputs=input_layer)\n",
    "\n",
    "        # create a placeholder for an encoded input\n",
    "        encoded_input = Input(shape=(self.dim,))\n",
    "        # retrieve the last layer of the autoencoder model\n",
    "        decoder_layer = self.autoencoder.layers[-1]\n",
    "        # create the decoder model\n",
    "        self.decoder = Model(outputs=decoder_layer(encoded_input), inputs=encoded_input)\n",
    "#removing x_val from this call \n",
    "    def fit(self, train_X, nb_epoch=50, batch_size=100, contractive=None):\n",
    "        optimizer = Adadelta(lr=2.)\n",
    "        # optimizer = Adam()\n",
    "        # optimizer = Adagrad()\n",
    "        if contractive:\n",
    "            print('Using contractive loss, lambda: %s' % contractive)\n",
    "            self.autoencoder.compile(optimizer=optimizer, loss=contractive_loss(self, contractive))\n",
    "        else:\n",
    "            print('Using binary crossentropy')\n",
    "            self.autoencoder.compile(optimizer=optimizer, loss='binary_crossentropy') # kld, binary_crossentropy, mse\n",
    "\n",
    "        self.autoencoder.fit(train_X[0], train_X[1],\n",
    "                        epochs=nb_epoch,\n",
    "                        batch_size=batch_size,\n",
    "                        #shuffle=True,\n",
    "                        shuffle=True\n",
    "                      #  validation_data=(val_X[0], val_X[1]),\n",
    "                        #callbacks=[\n",
    "                        #            ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.01),\n",
    "                        #            EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=5, verbose=1, mode='auto'),\n",
    "                       #            CustomModelCheckpoint(self.encoder, self.save_model, monitor='val_loss', save_best_only=True, mode='auto')\n",
    "                        #]\n",
    "                        )\n",
    "\n",
    "        return self\n",
    "\n",
    "def save_ae_model(model, model_file):\n",
    "    model.save(model_file)\n",
    "\n",
    "def load_ae_model(model_file):\n",
    "    return load_keras_model(model_file, custom_objects={\"KCompetitive\": KCompetitive})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Replaces autoencoder.preprocessing \n",
    "\n",
    "'''\n",
    "Created on Dec, 2016\n",
    "\n",
    "@author: hugo\n",
    "\n",
    "'''\n",
    "from __future__ import absolute_import\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import codecs\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.porter import PorterStemmer as EnglishStemmer\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#from ..utils.io_utils import dump_json, load_json, write_file\n",
    "\n",
    "\n",
    "def load_stopwords(file):\n",
    "    stop_words = []\n",
    "    try:\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                stop_words.append(line.strip('\\n '))\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    return stop_words\n",
    "\n",
    "def init_stopwords():\n",
    "    try:\n",
    "        stopword_path = 'patterns/english_stopwords.txt'\n",
    "        cached_stop_words = load_stopwords(os.path.join(os.path.split(__file__)[0], stopword_path))\n",
    "        print('Loaded %s' % stopword_path)\n",
    "    except:\n",
    "        from nltk.corpus import stopwords\n",
    "        cached_stop_words = stopwords.words(\"english\")\n",
    "        print('Loaded nltk.corpus.stopwords')\n",
    "\n",
    "    return cached_stop_words\n",
    "\n",
    "def tiny_tokenize(text, stem=False, stop_words=[]):\n",
    "    words = []\n",
    "    for token in wordpunct_tokenize(re.sub('[%s]' % re.escape(string.punctuation), ' ', \\\n",
    "            text.decode(encoding='UTF-8', errors='ignore'))):\n",
    "        if not token.isdigit() and not token in stop_words:\n",
    "            if stem:\n",
    "                try:\n",
    "                    w = EnglishStemmer().stem(token)\n",
    "                except Exception as e:\n",
    "                    w = token\n",
    "            else:\n",
    "                w = token\n",
    "            words.append(w)\n",
    "\n",
    "    return words\n",
    "\n",
    "    # return [EnglishStemmer().stem(token) if stem else token for token in wordpunct_tokenize(\n",
    "    #                     re.sub('[%s]' % re.escape(string.punctuation), ' ', text.decode(encoding='UTF-8', errors='ignore'))) if\n",
    "    #                     not token.isdigit() and not token in stop_words]\n",
    "\n",
    "def tiny_tokenize_xml(text, stem=False, stop_words=[]):\n",
    "    return [EnglishStemmer().stem(token) if stem else token for token in wordpunct_tokenize(\n",
    "                        re.sub('[%s]' % re.escape(string.punctuation), ' ', text.encode(encoding='ascii', errors='ignore'))) if\n",
    "                        not token.isdigit() and not token in stop_words]\n",
    "\n",
    "def get_all_files(corpus_path, recursive=False):\n",
    "    if recursive:\n",
    "        return [os.path.join(root, file) for root, dirnames, filenames in os.walk(corpus_path) for file in filenames if os.path.isfile(os.path.join(root, file)) and not file.startswith('.')]\n",
    "    else:\n",
    "        return [os.path.join(corpus_path, filename) for filename in os.listdir(corpus_path) if os.path.isfile(os.path.join(corpus_path, filename)) and not filename.startswith('.')]\n",
    "\n",
    "def count_words(docs):\n",
    "    # count the number of times a word appears in a corpus\n",
    "    word_freq = defaultdict(lambda: 0)\n",
    "    for each in docs:\n",
    "        for word, val in each.iteritems():\n",
    "            word_freq[word] += val\n",
    "\n",
    "    return word_freq\n",
    "\n",
    "def load_data(corpus_path, recursive=False, stem=False, stop_words=False):\n",
    "    word_freq = defaultdict(lambda: 0) # count the number of times a word appears in a corpus\n",
    "    doc_word_freq = defaultdict(dict) # count the number of times a word appears in a doc\n",
    "    files = get_all_files(corpus_path, recursive)\n",
    "\n",
    "    # word_tokenizer = RegexpTokenizer(r'[a-zA-Z]+') # match only alphabet characters\n",
    "    # cached_stop_words = init_stopwords()\n",
    "    cached_stop_words = init_stopwords() if stop_words else []\n",
    "\n",
    "    for filename in files:\n",
    "        try:\n",
    "            # with open(filename, 'r') as fp:\n",
    "            with codecs.open(filename, 'r', encoding='UTF-8', errors='ignore') as fp:\n",
    "                text = fp.read().lower()\n",
    "                # words = [word for word in word_tokenizer.tokenize(text) if word not in cached_stop_words]\n",
    "                # remove punctuations, stopwords and *unnecessary digits*, stemming\n",
    "                words = tiny_tokenize(text.decode('utf-8'), stem, cached_stop_words)\n",
    "\n",
    "                # doc_name = os.path.basename(filename)\n",
    "                parent_name, child_name = os.path.split(filename)\n",
    "                doc_name = os.path.split(parent_name)[-1] + '_' + child_name\n",
    "                for i in range(len(words)):\n",
    "                    # doc-word frequency\n",
    "                    try:\n",
    "                        doc_word_freq[doc_name][words[i]] += 1\n",
    "                    except:\n",
    "                        doc_word_freq[doc_name][words[i]] = 1\n",
    "                    # word frequency\n",
    "                    word_freq[words[i]] += 1\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    return word_freq, doc_word_freq\n",
    "\n",
    "def construct_corpus(corpus_path, training_phase, vocab_dict=None, threshold=5, topn=None, recursive=False):\n",
    "    if not (training_phase or isinstance(vocab_dict, dict)):\n",
    "        raise ValueError('vocab_dict must be provided if training_phase is set False')\n",
    "\n",
    "    word_freq, doc_word_freq = load_data(corpus_path, recursive)\n",
    "    if training_phase:\n",
    "        vocab_dict = build_vocab(word_freq, threshold=threshold, topn=topn)\n",
    "\n",
    "    docs = generate_bow(doc_word_freq, vocab_dict)\n",
    "    new_word_freq = dict([(vocab_dict[word], freq) for word, freq in word_freq.iteritems() if word in vocab_dict])\n",
    "\n",
    "    return docs, vocab_dict, new_word_freq\n",
    "\n",
    "def load_corpus(corpus_path):\n",
    "    corpus = load_json(corpus_path)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "def generate_bow(doc_word_freq, vocab_dict):\n",
    "    docs = {}\n",
    "    for key, val in doc_word_freq.iteritems():\n",
    "        word_count = {}\n",
    "        for word, freq in val.iteritems():\n",
    "            try:\n",
    "                word_count[vocab_dict[word]] = freq\n",
    "            except: # word is not in vocab, i.e., this word should be discarded\n",
    "                continue\n",
    "        docs[key] = word_count\n",
    "\n",
    "    return docs\n",
    "\n",
    "def build_vocab(word_freq, threshold=5, topn=None, start_idx=0):\n",
    "    \"\"\"\n",
    "    threshold only take effects when topn is None.\n",
    "    words are indexed by overall frequency in the dataset.\n",
    "    \"\"\"\n",
    "    word_freq = sorted(word_freq.iteritems(), key=lambda d:d[1], reverse=True)\n",
    "    if topn:\n",
    "        word_freq = zip(*word_freq[:topn])[0]\n",
    "        vocab_dict = dict(zip(word_freq, range(start_idx, len(word_freq) + start_idx)))\n",
    "    else:\n",
    "        idx = start_idx\n",
    "        vocab_dict = {}\n",
    "        for word, freq in word_freq:\n",
    "            if freq < threshold:\n",
    "                return vocab_dict\n",
    "            vocab_dict[word] = idx\n",
    "            idx += 1\n",
    "    return vocab_dict\n",
    "\n",
    "def construct_train_test_corpus(train_path, test_path, output, threshold=5, topn=None):\n",
    "    train_docs, vocab_dict, train_word_freq = construct_corpus(train_path, True, threshold=threshold, topn=topn, recursive=True)\n",
    "    train_corpus = {'docs': train_docs, 'vocab': vocab_dict, 'word_freq': train_word_freq}\n",
    "    dump_json(train_corpus, os.path.join(output, 'train.corpus'))\n",
    "    print('Generated training corpus')\n",
    "\n",
    "    test_docs, _, _ = construct_corpus(test_path, False, vocab_dict=vocab_dict, recursive=True)\n",
    "    test_corpus = {'docs': test_docs, 'vocab': vocab_dict}\n",
    "    dump_json(test_corpus, os.path.join(output, 'test.corpus'))\n",
    "    print('Generated test corpus')\n",
    "\n",
    "    return train_corpus, test_corpus\n",
    "\n",
    "def corpus2libsvm(docs, doc_labels, output):\n",
    "    '''Convert the corpus format to libsvm format.\n",
    "    '''\n",
    "    data = []\n",
    "    names = []\n",
    "    for key, val in docs.iteritems():\n",
    "        # label = doc_labels[key]\n",
    "        label = 0\n",
    "        line = label if isinstance(label, list) else [str(label)] + [\"%s:%s\" % (int(x) + 1, y) for x, y in val.iteritems()]\n",
    "        data.append(line)\n",
    "        names.append(key)\n",
    "    write_file(data, output)\n",
    "    write_file(names, output + '.fnames')\n",
    "    return data, names\n",
    "\n",
    "def doc2vec(doc, dim):\n",
    "    vec = np.zeros(dim)\n",
    "    for idx, val in doc.items():\n",
    "        vec[int(idx)] = val\n",
    "\n",
    "    return vec\n",
    "\n",
    "def idf(docs, dim):\n",
    "    vec = np.zeros((dim, 1))\n",
    "    for each_doc in docs:\n",
    "        for idx in each_doc.keys():\n",
    "            vec[int(idx)] += 1\n",
    "\n",
    "    return np.log10(1. + len(docs) / vec)\n",
    "\n",
    "def vocab_weights(vocab_dict, word_freq, max_=100., ratio=.75):\n",
    "    weights = np.zeros((len(vocab_dict), 1))\n",
    "\n",
    "    for word, idx in vocab_dict.items():\n",
    "        weights[idx] = word_freq[str(idx)]\n",
    "    weights = np.clip(weights / max_, 0., 1.)\n",
    "\n",
    "    return np.power(weights, ratio)\n",
    "\n",
    "def vocab_weights_tfidf(vocab_dict, word_freq, docs, max_=100., ratio=.75):\n",
    "    dim = len(vocab_dict)\n",
    "    tf_vec = np.zeros((dim, 1))\n",
    "    for word, idx in vocab_dict.items():\n",
    "        tf_vec[idx] = 1. + np.log10(word_freq[idx]) # log normalization\n",
    "\n",
    "    idf_vec = idf(docs, dim)\n",
    "    tfidf_vec = tf_vec * idf_vec\n",
    "\n",
    "    tfidf_vec = np.clip(tfidf_vec, 0., 4.)\n",
    "    return np.power(tfidf_vec, ratio)\n",
    "\n",
    "# # Init weights with topic modeling results\n",
    "# def init_weights(topic_vocab_dist, vocab_dict, epsilon=1e-5):\n",
    "#     weights = np.zeros((len(vocab_dict), len(topic_vocab_dist)))\n",
    "#     for i in range(len(topic_vocab_dist)):\n",
    "#         for k, v in topic_vocab_dist[i]:\n",
    "#             weights[vocab_dict[k]][i] = 1. + epsilon\n",
    "\n",
    "#     return weights\n",
    "\n",
    "# def init_weights2(topic_vocab, vocab_dict, epsilon=1e-5):\n",
    "#     weights = np.zeros((len(vocab_dict), len(topic_vocab)))\n",
    "#     for i in range(len(topic_vocab)):\n",
    "#         for vocab in topic_vocab[i]:\n",
    "#             weights[vocab_dict[vocab]][i] = 1. / len(topic_vocab[i]) + epsilon\n",
    "\n",
    "#     return weights\n",
    "\n",
    "def generate_20news_doc_labels(doc_names, output):\n",
    "    doc_labels = {}\n",
    "    for each in doc_names:\n",
    "        label = each.split('_')[0]\n",
    "        doc_labels[each] = label\n",
    "\n",
    "    dump_json(doc_labels, output)\n",
    "\n",
    "    return doc_labels\n",
    "\n",
    "def generate_8k_doc_labels(doc_names, output):\n",
    "    doc_labels = {}\n",
    "    for each in doc_names:\n",
    "        label = each.split('_')[-1].replace('.txt', '')\n",
    "        doc_labels[each] = label\n",
    "\n",
    "    dump_json(doc_labels, output)\n",
    "\n",
    "    return doc_labels\n",
    "\n",
    "def get_8k_doc_bnames(doc_names):\n",
    "    doc_labels = {}\n",
    "    for doc in doc_names:\n",
    "        doc_labels[doc] = doc.split('-')[-1].replace('.txt', '')\n",
    "\n",
    "    return doc_labels\n",
    "\n",
    "def get_8k_doc_years(doc_names):\n",
    "    doc_labels = {}\n",
    "    for doc in doc_names:\n",
    "        doc_labels[doc] = doc.split('-')[0]\n",
    "\n",
    "    return doc_labels\n",
    "\n",
    "def get_8k_doc_fails(doc_names, bank_fyear):\n",
    "    doc_labels = {}\n",
    "    for doc in doc_names:\n",
    "        fyear = bank_fyear[doc.split('-')[-1].replace('.txt', '')]\n",
    "        doc_labels[doc] = 1 if fyear != 'NA' and abs(int(doc.split('-')[0]) - int(fyear)) <= 1 else 0\n",
    "\n",
    "    return doc_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Prepare data NLPCF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "import gensim\n",
    "\n",
    "os.chdir(\"/home/spenser/Downloads/case_study\")\n",
    "\n",
    "CFPB_1 = pd.read_csv(\"cfpb_1.csv\")\n",
    "\n",
    "CFPB_2 = pd.read_csv(\"cfpb_2.csv\", header = None)\n",
    "CFPB_2.columns = ['complaint_id', 'text']\n",
    "\n",
    "CFPB_3 = pd.read_csv(\"cfpb_3.csv\", header = None)\n",
    "CFPB_3.columns = ['complaint_id', 'text']\n",
    "\n",
    "CFPB_4 = pd.read_csv(\"cfpb_4.csv\", header = None)\n",
    "CFPB_4.columns = ['complaint_id', 'text']\n",
    "\n",
    "CFPB_5 = pd.read_csv(\"cfpb_5.csv\", header = None)\n",
    "CFPB_5.columns = ['complaint_id', 'text']\n",
    "\n",
    "CFPB_text = pd.concat([CFPB_1, CFPB_2, CFPB_3, CFPB_4, CFPB_5])\n",
    "\n",
    "file_no_text = pd.read_csv(\"cfpb_triage_case_study_notext.csv\")\n",
    "\n",
    "CFPB_prod = pd.read_csv(\"Consumer_Complaints.csv\")\n",
    "\n",
    "#names = full_CFPB_data.columns.tolist()\n",
    "#names[names.index('Complaint ID')] = 'complaint_id'\n",
    "#full_CFPB_data.columns = names\n",
    "\n",
    "CFPB_Case_Study_Joined = file_no_text.merge(CFPB_text, on = 'complaint_id', how ='left')\n",
    "\n",
    "import string \n",
    "#text already appears to be cleansed ... but, just making sure. limited pre-processing - these assumptions can be played around with when we build classifiers\n",
    "\n",
    "CFPB_Case_Study_Joined[\"text_lower\"] = CFPB_Case_Study_Joined[\"text\"].str.lower()\n",
    "\n",
    "CFPB_Case_Study_Joined[\"text_lower\"] = CFPB_Case_Study_Joined[\"text_lower\"].str.replace(r'\\nRevision: (\\d+)\\n', '') #remove digits\n",
    "\n",
    "def remove_punctuations(text):\n",
    "\n",
    "    for punctuation in string.punctuation:\n",
    "\n",
    "        text = text.replace(punctuation, '')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "CFPB_Case_Study_Joined[\"text_clean\"] = CFPB_Case_Study_Joined['text_lower'].apply(remove_punctuations)  #remove punctuation\n",
    "\n",
    "\n",
    "#Adding this due to finding below that pre-cleansed text is corrupt. (contains cases like can t) . Remove single stand-alone characters. (\"a\", \"e\", etc)\n",
    "\n",
    "CFPB_Case_Study_Joined[\"text_clean\"] = CFPB_Case_Study_Joined[\"text_clean\"].str.replace(r'\\b(?<=)[a-z](?=)\\b', '') #remove single stand-alone characters.\n",
    "\n",
    " \n",
    "cfpb_pd = CFPB_Case_Study_Joined\n",
    "\n",
    "cfpb_pd[\"text_clean\"] = cfpb_pd[\"text_clean\"].str.replace(r'xx', '')\n",
    "cfpb_pd[\"text_clean\"] = cfpb_pd[\"text_clean\"].str.replace(r'(.)\\1{2,}', '')\n",
    "\n",
    "cfpb_pd[\"word_count\"] = cfpb_pd[\"text_clean\"].str.count(' ') + 1\n",
    "\n",
    "cfpb_pd = cfpb_pd[cfpb_pd[\"word_count\"] >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfpb_pd_sample = cfpb_pd.sample(80000, random_state=42)\n",
    "\n",
    "fullcorpus = ' '.join(cfpb_pd_sample[\"text_clean\"]) ###Determine vocabulary size\n",
    "\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, one_hot, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "words = set(text_to_word_sequence(fullcorpus))\n",
    "\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "fullcorpus = []\n",
    "CFPB_Case_Study_Joined = []\n",
    "cfpb_pd = []\n",
    "CFPD_text = []\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prepare bag of words vectorization for model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "#TFIDF (Term Frequency Inverse Document Frequency will provide relative importance weightings for terms, and\n",
    "# de-emphasize more common terms ... this should help with the problems noted above.)\n",
    "import gc\n",
    "gc.collect()\n",
    "#without going through and manually picking out stop-words  like time, ask, etc, as stop words that seem to be driving the\n",
    "#above categories \n",
    "\n",
    "#tfidf_vectorizer = TfidfVectorizer()\n",
    "#tfidf = tfidf_vectorizer.fit_transform(bank_service_train[\"sparse\"])\n",
    "\n",
    "#tfidf_array = tfidf.toarray()\n",
    "#df = tfidf_array\n",
    "#Use silhouette score                                               \n",
    "def cv(data):\n",
    "    cv_vectorizer = CountVectorizer(min_df=40)\n",
    "    train = cv_vectorizer.fit_transform(data)\n",
    "    return train, cv_vectorizer\n",
    "\n",
    "cv_array_sparse, cv_vectorizer = cv(cfpb_pd_sample[\"text_clean\"])\n",
    "#tfidf_array = tfidf_array_sparse.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_array = cv_array_sparse.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_array_sparse = []\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Sequencing and Padding Text Inputs for Input to embedding layer\n",
    "from sklearn.pipeline import make_pipeline, TransformerMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "#feature_size = len(word_set)\n",
    "maxlen = 8000\n",
    "maxlen = 8000\n",
    "max_review_length = 8000\n",
    "#fullcorpus = ' '.join(Train_TfIDF_Clusters_Products['text_clean_banks']) ###Determine vocabulary size\n",
    "\n",
    "\n",
    "\n",
    "#class TextsToSequences(Tokenizer, BaseEstimator, TransformerMixin, lower = False):\n",
    "#    def __init__(self,  **kwargs):\n",
    "\n",
    " #       super().__init__(**kwargs)\n",
    "\n",
    " #   def fit(self, texts, y=None):\n",
    "   #     self.fit_on_texts(texts)\n",
    "   #     return self\n",
    "\n",
    "   # def transform(self, texts, y = None):\n",
    "\n",
    "     #   return np.array(self.texts_to_sequences(texts))\n",
    "    \n",
    "    \n",
    "\n",
    "class TextsToSequences(Tokenizer, BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self,  **kwargs):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, texts, y=None):\n",
    "\n",
    "        self.fit_on_texts(texts)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "    def transform(self, texts, y = None):\n",
    "\n",
    "        return np.array(self.texts_to_sequences(texts))\n",
    "        \n",
    "    \n",
    "# = feature size without banks\n",
    "sequencer = TextsToSequences(num_words=vocab_size)\n",
    "\n",
    "### Function to pad sequences. Padding constrains all input vectors to be the same length.\n",
    "\n",
    "class Padder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, maxlen=maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_index = None\n",
    "    def fit(self, X, y=None):\n",
    "        self.max_indent = pad_sequences(X, maxlen=self.maxlen).max()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = pad_sequences(X, maxlen=self.maxlen)\n",
    "                    #X[X > self.max_index] = 0\n",
    "        return X\n",
    "padder = Padder(maxlen)\n",
    "\n",
    "#feature_size = 80826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.array(cfpb_pd[\"text_clean\"][0:2])\n",
    "test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_full = []\n",
    "for doc in cfpb_pd[\"text_clean\"]:\n",
    "    words = doc.split()\n",
    "    for word in words:\n",
    "        word_list_full.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_lf = set([word for word in word_list_full])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from pprint import pprint  # pretty-printer\n",
    ">>> from collections import defaultdict\n",
    ">>>\n",
    ">>> # remove common words and tokenize\n",
    ">>> stoplist = set('for a of the and to in'.split())\n",
    ">>> texts = [\n",
    ">>>     [word for word in document.lower().split()]\n",
    ">>>     for document in cfpb_pd[\"text_clean\"]\n",
    ">>> ]\n",
    ">>>\n",
    ">>> # remove words that appear only once\n",
    ">>> frequency = defaultdict(int)\n",
    ">>> for text in texts:\n",
    ">>>     for token in text:\n",
    ">>>         frequency[token] += 1\n",
    ">>>\n",
    ">>> texts = [\n",
    ">>>     [token for token in text if frequency[token] > 1]\n",
    ">>>     for text in texts\n",
    ">>> ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    ">>> dictionary = corpora.Dictionary(texts)\n",
    "\n",
    ">>> print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    ">>> corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)  # store to disk, for later use\n",
    ">>> print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_array = np.array(cfpb_pd[\"text_clean\"][0:6])\n",
    "pipeline1 = make_pipeline(sequencer)\n",
    "pipeline1.fit_transform(texts_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_docs = sequencer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on Nov, 2016\n",
    "\n",
    "@author: hugo\n",
    "\n",
    "'''\n",
    "from __future__ import absolute_import\n",
    "import timeit\n",
    "import argparse\n",
    "from os import path\n",
    "import numpy as np\n",
    "\n",
    "#from autoencoder.core.ae import AutoEncoder, load_ae_model, save_ae_model replaced cell \n",
    "#from autoencoder.preprocessing.preprocessing import load_corpus, doc2vec\n",
    "#from autoencoder.utils.op_utils import vecnorm, add_gaussian_noise, add_masking_noise, add_salt_pepper_noise replace cell\n",
    "#from autoencoder.utils.io_utils import dump_json\n",
    "\n",
    "\n",
    "#def train(args):\n",
    "#corpus = load_corpus(args.input)\n",
    "\n",
    "\n",
    "n_vocab, docs = len(corpus['vocab']), corpus['docs']\n",
    "corpus.clear() # save memory\n",
    "doc_keys = docs.keys()\n",
    "#X_docs = []\n",
    "#for k in doc_keys:\n",
    "#    X_docs.append(vecnorm(doc2vec(docs[k], n_vocab), 'logmax1', 0))\n",
    "#    del docs[k]\n",
    "\n",
    "###X_docs could be replaced by doc2vec ? \n",
    "#n_vocab\n",
    "#X_docs = pd.read_csv(\"/home/spenser/u_pd_neighbor20_mindist0.05_30components.csv\")\n",
    "#X_docs = np.r_[X_docs]\n",
    "\n",
    "if args.noise == 'gs':\n",
    "    X_docs_noisy = add_gaussian_noise(X_docs, 0.1)\n",
    "elif args.noise == 'sp':\n",
    "    X_docs_noisy = add_salt_pepper_noise(X_docs, 0.1)\n",
    "    pass\n",
    "elif args.noise == 'mn':\n",
    "    X_docs_noisy = add_masking_noise(X_docs, 0.01)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "n_samples = X_docs.shape[0]\n",
    "np.random.seed(0)\n",
    "val_idx = np.random.choice(range(n_samples), args.n_val, replace=False)\n",
    "train_idx = list(set(range(n_samples)) - set(val_idx))\n",
    "X_train = X_docs[train_idx]\n",
    "X_val = X_docs[val_idx]\n",
    "del X_docs\n",
    "\n",
    "if args.noise:\n",
    "    # X_train_noisy = X_docs_noisy[:-n_val]\n",
    "    # X_val_noisy = X_docs_noisy[-n_val:]\n",
    "    X_train_noisy = X_docs_noisy[train_idx]\n",
    "    X_val_noisy = X_docs_noisy[val_idx]\n",
    "    print('added %s noise' % args.noise)\n",
    "else:\n",
    "    X_train_noisy = X_train\n",
    "    X_val_noisy = X_val\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "ae = AutoEncoder(n_vocab, args.n_dim, comp_topk=args.comp_topk, ctype=args.ctype, save_model=args.save_model)\n",
    "ae.fit([X_train_noisy, X_train], [X_val_noisy, X_val], nb_epoch=args.n_epoch, \\\n",
    "        batch_size=args.batch_size, contractive=args.contractive)\n",
    "\n",
    "print('runtime: %ss' % (timeit.default_timer() - start))\n",
    "\n",
    " #   if args.output:\n",
    " #       train_doc_codes = ae.encoder.predict(X_train)\n",
    "  #      val_doc_codes = ae.encoder.predict(X_val)\n",
    "  #      doc_keys = np.array(doc_keys)\n",
    "  #      dump_json(dict(zip(doc_keys[train_idx].tolist(), train_doc_codes.tolist())), args.output + '.train')\n",
    "  #      dump_json(dict(zip(doc_keys[val_idx].tolist(), val_doc_codes.tolist())), args.output + '.val')\n",
    "  #      print('Saved doc codes file to %s and %s' % (args.output + '.train', args.output + '.val'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#else:\n",
    "n_val = 20\n",
    "#    pass\n",
    "n_samples = cv_array.shape[0]\n",
    "np.random.seed(0)\n",
    "val_idx = np.random.choice(range(n_samples), n_val, replace=False)\n",
    "train_idx = list(set(range(n_samples)) - set(val_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train = cv_array[train_idx]\n",
    "X_val = cv_array[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_docs_noisy = add_gaussian_noise(cv_array, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_noisy = X_docs_noisy[train_idx]\n",
    "X_val_noisy = X_docs_noisy[val_idx]\n",
    "\n",
    "cv_array = []\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_docs_noisy = []\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "X_train = normalize(X_train, norm='l1', axis=1)\n",
    "X_train_noisy = normalize(X_train_noisy, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AutoEncoder(6255, dim = 300, comp_topk=None, ctype='kcomp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.fit([X_train_noisy, X_train], [X_val_noisy, X_val], nb_epoch=20,  batch_size=100, contractive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = ae.encoder.predict(X_train, batch_size=100)\n",
    "#plt.figure(figsize=(6, 6))\n",
    "#plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1])\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "print(x_test_encoded.shape)\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(x_test_encoded)  \n",
    "print(pca.explained_variance_ratio_)  \n",
    "\n",
    "print(pca.singular_values_)  \n",
    "\n",
    "X_pca = pca.transform(x_test_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "#from hdbscan import HDBSCAN\n",
    "\n",
    "from hdbscan.hdbscan_ import HDBSCAN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = pd.read_csv(\"/home/spenser/product.csv\")\n",
    "product = product.astype(str)\n",
    "product.columns = [\"products\"]\n",
    "productc = pd.Categorical(product[\"products\"]).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def shuffle(df, n=1, axis=0):     \n",
    "    df = df.copy()\n",
    "    for _ in range(n):\n",
    "        df.apply(np.random.shuffle, axis=axis)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=X_pca[:,0],\n",
    "    y=X_pca[:,1],\n",
    "    z=X_pca[:,2],\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        cmax=5,\n",
    "        cmin=0,\n",
    "        color=productc,\n",
    "        colorbar=dict(\n",
    "            title=\"Colorbar\"\n",
    "        ),\n",
    "        colorscale=\"mygbm\"\n",
    "    ),\n",
    "    mode=\"markers\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"/home/spenser/Downloads/case_study/doc2vec_300d_model3_pd.csv\")\n",
    "X_docs = []\n",
    "#else:\n",
    "X_train = np.array(X_train)\n",
    "#X_val = X_docs[val_idx]\n",
    "\n",
    "X_train_noisy = add_gaussian_noise(X_train, 0.1)\n",
    "\n",
    "X_docs_noisy = []\n",
    "\n",
    "cv_array = []\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_noisy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AutoEncoder(300, dim = 3, comp_topk=None, ctype='kcomp')\n",
    "ae.fit([X_train_noisy, X_train], nb_epoch=30 ,batch_size=100, contractive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = ae.encoder.predict(X_train, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test_encoded = ae.encoder.predict(X_train, batch_size=100)\n",
    "#plt.figure(figsize=(6, 6))\n",
    "#plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1])\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(x_test_encoded)\n",
    "X_pca = pca.transform(x_test_encoded)  \n",
    "\n",
    "print(pca.explained_variance_ratio_)  \n",
    "\n",
    "print(pca.singular_values_)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=X_pca[:,0],\n",
    "    y=X_pca[:,1],\n",
    "    z=X_pca[:,2],\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        cmax=5,\n",
    "        cmin=0,\n",
    "        color=productc,\n",
    "        colorbar=dict(\n",
    "            title=\"Colorbar\"\n",
    "        ),\n",
    "        colorscale=\"mygbm\"\n",
    "    ),\n",
    "    mode=\"markers\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plt.figure(figsize=(6, 6))\n",
    "#plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1])\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X_train)\n",
    "X_pca = pca.transform(X_train)  \n",
    "\n",
    "print(pca.explained_variance_ratio_)  \n",
    "\n",
    "print(pca.singular_values_)  \n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=X_pca[:,0],\n",
    "    y=X_pca[:,1],\n",
    "    z=X_pca[:,2],\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        cmax=5,\n",
    "        cmin=0,\n",
    "        color=productc,\n",
    "        colorbar=dict(\n",
    "            title=\"Colorbar\"\n",
    "        ),\n",
    "        colorscale=\"mygbm\"\n",
    "    ),\n",
    "    mode=\"markers\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
